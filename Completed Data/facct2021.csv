year,doi_link,title,category,abstract,keywords,CCS concepts,author_names,affiliations,affiliated_countries
2021,https://dl.acm.org/doi/10.1145/3442188.3445938,An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,Transparency & Explainability,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","['Participatory design', 'participatory action research', 'accountability', 'algorithmic equity', 'algorithmic justice', 'surveillance', 'regulation']","['Social and professional topics _ Surveillance', 'Governmental regulations', 'Computing literacy', 'Human-centered computing _ Participatory design', 'Computing methodologies _ Artificial intelligence']","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']","['Creative Computing Institute, University of Arts London', 'Digital Life Initiative, Cornell Tech', 'Public Policy Programme, Alan Turing Institute', 'ACLU of Washington', 'MIRA', 'Coveillance Collective', 'Human Centered Design & Engineering, University of Washington', 'eScience Institute, University of Washington', 'Evans School of Public Policy & Governance, University of Washington', 'Department of Sociology, West Virginia University', 'Department of Computer Science, Middlebury College', 'School of Information, University of Michigan', 'Albers School of Business & Economics, Seattle University', 'Oxford Department of International Development, University of Oxford', 'ACLU of Washington', 'ACLU of Washington']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445928,Building and Auditing Fair Algorithms: A Case Study in Candidate Screening,Transparency & Explainability,"Academics, activists, and regulators are increasingly urging companies to develop and deploy sociotechnical systems that are fair and unbiased. Achieving this goal, however, is complex: the developer must (1) deeply engage with social and legal facets of ""fairness"" in a given context, (2) develop software that concretizes these values, and (3) undergo an independent algorithm audit to ensure technical correctness and social accountability of their algorithms. To date, there are few examples of companies that have transparently undertaken all three steps. In this paper we outline a framework for algorithmic auditing by way of a case-study of pymetrics, a startup that uses machine learning to recommend job candidates to their clients. We discuss how pymetrics approaches the question of fairness given the constraints of ethical, regulatory, and client demands, and how pymetrics' software implements adverse impact testing. We also present the results of an independent audit of pymetrics' candidate screening tool. We conclude with recommendations on how to structure audits to be practical, independent, and constructive, so that companies have better incentive to participate in third party audits, and that watchdog groups can be better prepared to investigate companies.","['algorithm auditing', 'four-fifths rule', 'adverse impact testing', 'fairness']","['Social and professional topics _ Gender', 'Race and ethnicity', 'Employment issues', 'Codes of ethics']","['Christo Wilson', 'Avijit Ghosh', 'Shan Jiang', 'Alan Mislove', 'Lewis Baker', 'Janelle Szary', 'Kelly Trindel', 'Frida Polli']","['Northeastern University', 'Northeastern University', 'Northeastern University', 'Northeastern University', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.', 'pymetrics, inc.']","[None, None, None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445881,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,Transparency & Explainability,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",[],[],"['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","[None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445866,"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable AI ",Transparency & Explainability,"The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.","['Explainable AI', 'Explainable Artificial Intelligence', 'Explainable Machine Learning', 'Interpretable Machine Learning', 'Ethics of AI', 'Ethical AI', 'Machine learning', 'Philosophy of Explanation', 'Philosophy of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning', 'Human-centered computing']",['Atoosa Kasirzadeh'],['University of Toronto Australian National University'],[None]
2021,https://dl.acm.org/doi/10.1145/3442188.3445917,Impossible Explanations? Beyond explainable AI in the GDPR from a COVID-19 use case scenario,Transparency & Explainability,"Can we achieve an adequate level of explanation for complex machine learning models in high-risk AI applications when applying the EU data protection framework? In this article, we address this question, analysing from a multidisciplinary point of view the connection between existing legal requirements for the explainability of AI systems and the current state of the art in the field of explainable AI. We present a case study of a real-life scenario designed to illustrate the application of an AI-based automated decision making process for the medical diagnosis of COVID-19 patients. The scenario exemplifies the trend in the usage of increasingly complex machine-learning algorithms with growing dimensionality of data and model parameters. Based on this setting, we analyse the challenges of providing human legible explanations in practice and we discuss their legal implications following the General Data Protection Regulation (GDPR). Although it might appear that there is just one single form of explanation in the GDPR, we conclude that the context in which the decision-making system operates requires that several forms of explanation are considered. Thus, we propose to design explanations in multiple forms, depending on: the moment of the disclosure of the explanation (either ex ante or ex post); the audience of the explanation (explanation for an expert or a data controller and explanation for the final data subject); the layer of granularity (such as general, group-based or individual explanations); the level of the risks of the automated decision regarding fundamental rights and freedoms. Consequently, explanations should embrace this multifaceted environment. Furthermore, we highlight how the current inability of complex, deep learning based machine learning models to make clear causal links between input data and final decisions represents a limitation for providing exact, human-legible reasons behind specific decisions. This makes the provision of satisfactorily, fair and transparent explanations a serious challenge. Therefore, there are cases where the quality of possible explanations might not be assessed as an adequate safeguard for automated decision-making processes under Article 22(3) GDPR. Accordingly, we suggest that further research should focus on alternative tools in the GDPR (such as algorithmic impact assessments from Article 35 GDPR or algorithmic lawfulness justifications) that might be considered to complement the explanations of automated decision-making.","['Explainability', 'AI', 'Machine Learning', 'GDPR', 'Black-Box', 'Automated Decision-Making', 'Data Protection']","['Applied computing _ Law, social and behavioral science', 'Computing methodologies _ Machine learning']","['Ronan Hamon', 'Henrik Junklewitz', 'Gianclaudio Malgieri', 'Paul De Hert', 'Laurent Beslay', 'Ignacio Sanchez']","['European Commission, Joint Research Centre, Ispra', 'European Commission, Joint Research Centre, Ispra', 'Augmented Law Institute, EDHEC Business School, Lille', 'Law Science Technology & Society, Vrije Universiteit Brussel, Brussels', 'European Commission, Joint Research Centre, Ispra', 'European Commission, Joint Research Centre, Ispra']","['Italy', 'Italy', 'France', 'Belgium', 'Italy', 'Italy']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445925,TILT: A GDPR-Aligned Transparency Information Language and Toolkit for Practical Privacy Engineering,Transparency & Explainability,"In this paper, we present TILT, a transparency information language and toolkit explicitly designed to represent and process transparency information in line with the requirements of the GDPR and allowing for a more automated and adaptive use of such information than established, legalese data protection policies do. We provide a detailed analysis of transparency obligations from the GDPR to identify the expressiveness required for a formal transparency language intended to meet respective legal requirements. In addition, we identify a set of further, non-functional requirements that need to be met to foster practical adoption in real-world (web) information systems engineering. On this basis, we specify our formal language and present a respective, fully implemented toolkit around it. We then evaluate the practical applicability of our language and toolkit and demonstrate the additional prospects it unlocks through two different use cases: a) the inter-organizational analysis of personal data-related practices allowing, for instance, to uncover data sharing networks based on explicitly announced transparency information and b) the presentation of formally represented transparency information to users through novel, more comprehensible, and potentially adaptive user interfaces, heightening data subjects' actual informedness about data-related practices and, thus, their sovereignty. Altogether, our transparency information language and toolkit allow - differently from previous work - to express transparency information in line with actual legal requirements and practices of modern (web) information systems engineering and thereby pave the way for a multitude of novel possibilities to heighten transparency and user sovereignty in practice.","['Data transparency', 'GDPR', 'data protection', 'privacy', 'privacy by design', 'legal tech', 'privacy engineering', 'web privacy', 'privacy law']","['Applied computing _ Law', 'Information systems _ Information systems applications', 'Web data description languages', 'Software and its engineering _ Formal language definitions', 'Context specific languages', 'Security and privacy _ Privacy protections']","['Elias Grünewald', 'Frank Pallas']","['Technische Universität Berlin, Information Systems Engineering Research Group, Berlin', 'Technische Universität Berlin, Information Systems Engineering Research Group, Berlin']","['Germany', 'Germany']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445891,Algorithmic Fairness in Predicting Opioid Use Disorder using Machine Learning,Fairness & Bias,"There has been recent interest by payers, health care systems, and researchers in the development of machine learning and artificial intelligence models that predict an individual's probability of developing opioid use disorder. The scores generated by these algorithms can be used by physicians to tailor the prescribing of opioids for the treatment of pain, reducing or foregoing prescribing to individuals deemed to be at high risk, or increasing prescribing for patients deemed to be at low risk. This paper constructs a machine learning algorithm to predict opioid use disorder risk using commercially available claims data similar to those utilized in the development of proprietary opioid use disorder prediction algorithms. We study risk scores generated by the machine learning model in a setting with quasi-experimental variation in the likelihood that doctors prescribe opioids, generated by changes in the legal structure for monitoring physician prescribing. We find that machine-predicted risk scores do not appear to correlate at all with the individual-specific heterogeneous treatment effect of receiving opioids. The paper identifies a new source of algorithmic unfairness in machine learning applications for health care and precision medicine, arising from the researcher's choice of objective function. While precision medicine should guide physician treatment decisions based on the heterogeneous causal impact of a course of treatment for an individual, allocating treatments to individuals receiving the most benefit and recommending caution for those most likely to experience harmful side effects, ML models in health care are often trained on proxies like individual baseline risk, and are not necessarily informative in deciding who would most benefit, or be harmed, by a course of treatment.","['causality', 'fairness', 'algorithm development', 'social and organizational processes', 'auditing', 'evaluations', 'ethics', 'algorithmic impacts']","['Applied computing _ Economics', 'Computing methodologies _ Machine learning', 'Social and professional topics _ Government technology policy']",['Angela E. Kilby'],"['Northeastern University Boston, Massachusetts']",['USA']
2021,https://dl.acm.org/doi/10.1145/3442188.3445887,Mitigating Bias in Set Selection with Noisy Protected Attributes,Fairness & Bias,"Subset selection algorithms are ubiquitous in AI-driven applications, including, online recruiting portals and image search engines, so it is imperative that these tools are not discriminatory on the basis of protected attributes such as gender or race. Currently, fair subset selection algorithms assume that the protected attributes are known as part of the dataset. However, protected attributes may be noisy due to errors during data collection or if they are imputed (as is often the case in real-world settings). While a wide body of work addresses the effect of noise on the performance of machine learning algorithms, its effect on fairness remains largely unexamined. We find that in the presence of noisy protected attributes, in attempting to increase fairness without considering noise, one can, in fact, decrease the fairness of the result! Towards addressing this, we consider an existing noise model in which there is probabilistic information about the protected attributes (e.g., [19, 32, 44, 56]), and ask is fair selection possible under noisy conditions? We formulate a ""denoised"" selection problem which functions for a large class of fairness metrics; given the desired fairness goal, the solution to the denoised problem violates the goal by at most a small multiplicative amount with high probability. Although this denoised problem turns out to be NP-hard, we give a linear-programming based approximation algorithm for it. We evaluate this approach on both synthetic and real-world datasets. Our empirical results show that this approach can produce subsets which significantly improve the fairness metrics despite the presence of noisy protected attributes, and, compared to prior noise-oblivious approaches, has better Pareto-tradeoffs between utility and fairness.",[],[],"['Anay Mehrotra', 'L. Elisa Celis']","['Yale University', 'Yale University']","[None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445889,Standardized Tests and Affirmative Action: The Role of Bias and Variance,Fairness & Bias,"The University of California suspended through 2024 the requirement that applicants from California submit SAT scores, upending the major role standardized testing has played in college admissions. We study the impact of such decisions and its interplay with other policies---such as affirmative action---on admitted class composition. This paper considers a theoretical framework to study the effect of requiring test scores on academic merit and diversity in college admissions. The model has a college and set of potential students. Each student has observed application components and group membership, as well as an unobserved noisy skill level generated from an observed distribution. The college is Bayesian and maximizes an objective that depends on both diversity and merit. It estimates each applicant's true skill level using the observed features and potentially their group membership, and then admits students with or without affirmative action. We characterize the trade-off between the (potentially positive) informational role of standardized testing in college admissions and its (negative) exclusionary nature. Dropping test scores may exacerbate disparities by decreasing the amount of information available for each applicant, especially those from non-traditional backgrounds. However, if there are substantial barriers to testing, removing the test improves both academic merit and diversity by increasing the size of the applicant pool. Finally, using application and transcript data from the University of Texas at Austin, we demonstrate how an admissions committee could measure the trade-off in practice to better decide whether to drop their test scores requirement. The full paper can be found at https://arxiv.org/abs/2010.04396.",[],[],"['Nikhil Garg', 'Hannah Li', 'Faidra Monachou']","['UC Berkeley, Berkeley', 'Stanford University, Stanford', 'Stanford University, Stanford']","['USA', 'USA', 'USA']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445875,Differential Tweetment: Mitigating Racial Dialect Bias in Harmful Tweet Detection,Fairness & Bias,"Automated systems for detecting harmful social media content are afflicted by a variety of biases, some of which originate in their training datasets. In particular, some systems have been shown to propagate racial dialect bias: they systematically classify content aligned with the African American English (AAE) dialect as harmful at a higher rate than content aligned with White English (WE). This perpetuates prejudice by silencing the Black community. Towards this problem we adapt and apply two existing bias mitigation approaches: preferential sampling pre-processing and adversarial debiasing in-processing. We analyse the impact of our interventions on model performance and propagated bias. We find that when bias mitigation is employed, a high degree of predictive accuracy is maintained relative to baseline, and in many cases bias against AAE in harmful tweet predictions is reduced. However, the specific effects of these interventions on bias and performance vary widely between dataset contexts. This variation suggests the unpredictability of autonomous harmful content detection outside of its development context. We argue that this, and the low performance of these systems at baseline, raise questions about the reliability and role of such systems in high-impact, real-world settings.","['bias, fairness', 'racial disparities', 'dialect', 'content moderation']","['Social and professional topics _ Race and ethnicity', 'Censorship', 'Computing methodologies _ Machine learning', 'Natural language processing']","['Ari Ball-Burack', 'Michelle Seng Ah Lee', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group University of Cambridge', 'Compliant & Accountable Systems Group University of Cambridge', 'Compliant & Accountable Systems Group University of Cambridge', 'Compliant & Accountable Systems Group University of Cambridge']","['UK', 'UK', 'UK', 'UK']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445912,Bridging Machine Learning and Mechanism Design towards Algorithmic Fairness,Fairness & Bias,"Decision-making systems increasingly orchestrate our world: how to intervene on the algorithmic components to build fair and equitable systems is therefore a question of utmost importance; one that is substantially complicated by the context-dependent nature of fairness and discrimination. Modern decision-making systems that involve allocating resources or information to people (e.g., school choice, advertising) incorporate machine-learned predictions in their pipelines, raising concerns about potential strategic behavior or constrained allocation, concerns usually tackled in the context of mechanism design. Although both machine learning and mechanism design have developed frameworks for addressing issues of fairness and equity, in some complex decision-making systems, neither framework is individually sufficient. In this paper, we develop the position that building fair decision-making systems requires overcoming these limitations which, we argue, are inherent to each field. Our ultimate objective is to build an encompassing framework that cohesively bridges the individual frameworks of mechanism design and machine learning. We begin to lay the ground work towards this goal by comparing the perspective each discipline takes on fair decision-making, teasing out the lessons each field has taught and can teach the other, and highlighting application domains that require a strong collaboration between these disciplines.",[],[],"['Jessie Finocchiaro', 'Roland Maio', 'Faidra Monachou', 'Gourab K Patro', 'Manish Raghavan', 'Ana-Andreea Stoica', 'Stratis Tsirtsis']","['CU Boulder', 'Columbia University', 'Stanford University', 'IIT Kharagpur', 'Cornell University', 'Columbia University', 'Max Planck Institute for Software Systems']","[None, None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445924,BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation ,Fairness & Bias,"Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.","['Fairness', 'natural language generation']",['Computing methodologies _ Natural language generation'],"['Jwala Dhamala', 'Tony Sun', 'Varun Kumar', 'Satyapriya Krishna', 'Yada Pruksachatkun', 'Kai-Wei Chang', 'Rahul Gupta']","['Amazon Alexa AI-NU', 'UC Santa Barbara', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU', 'Amazon Alexa AI-NU']","['USA', 'USA', 'USA', 'USA', 'USA', 'UCLA USA', 'USA']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445944,When the Umpire is also a Player: Bias in Private Label Product Recommendations on E-commerce Marketplaces,Fairness & Bias,"Algorithmic recommendations mediate interactions between millions of customers and products (in turn, their producers and sellers) on large e-commerce marketplaces like Amazon. In recent years, the producers and sellers have raised concerns about the fairness of black-box recommendation algorithms deployed on these marketplaces. Many complaints are centered around marketplaces biasing the algorithms to preferentially favor their own 'private label' products over competitors. These concerns are exacerbated as marketplaces increasingly de-emphasize or replace 'organic' recommendations with ad-driven 'sponsored' recommendations, which include their own private labels. While these concerns have been covered in popular press and have spawned regulatory investigations, to our knowledge, there has not been any public audit of these marketplace algorithms. In this study, we bridge this gap by performing an end-to-end systematic audit of related item recommendations on Amazon. We propose a network-centric framework to quantify and compare the biases across organic and sponsored related item recommendations. Along a number of our proposed bias measures, we find that the sponsored recommendations are significantly more biased toward Amazon private label products compared to organic recommendations. While our findings are primarily interesting to producers and sellers on Amazon, our proposed bias measures are generally useful for measuring link formation bias in any social or content networks.","['Recommendation', 'e-commerce marketplace', 'algorithmic auditing']",['Human-centered computing _ Empirical studies in collaborative and social computing'],"['Abhisek Dash', 'Abhijnan Chakraborty', 'Saptarshi Ghosh', 'Animesh Mukherjee', 'Krishna P. Gummadi']","['Indian Institute of Technology, Kharagpur', 'Indian Institute of Technology, Delhi', 'Indian Institute of Technology, Kharagpur', 'Indian Institute of Technology, Kharagpur', 'Max Planck Institute for Software, Systems']","['India', 'India', 'India', 'India', 'Germany']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445932,Image Representations Learned With Unsupervised Pre-Training Contain Human-like Biases,Fairness & Bias,"Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.","['implicit bias', 'unsupervised learning', 'computer vision']","['Computing methodologies _ Unsupervised learning', 'Transfer learning', 'Machine learning']","['Ryan Steed', 'Aylin Caliskan']","['Carnegie Mellon University, Pittsburgh, Pennsylvania', 'George Washington University, Washington']","['USA', 'USA']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445881,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,Fairness & Bias,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",[],[],"['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","[None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445893,"Spoken Corpora Data, Automatic Speech Recognition, and Bias Against African American Language: The case of Habitual 'Be'",Fairness & Bias,"Recent work has revealed that major automatic speech recognition (ASR) systems such as Apple, Amazon, Google, IBM, and Microsoft perform much more poorly for Black U.S. speakers than for white U.S. speakers. Researchers postulate that this may be a result of biased datasets which are largely racially homogeneous. However, while the study of ASR performance with regards to the intersection of racial identity and language use is slowly gaining traction within AI, machine learning, and algorithmic bias research, little to nothing has been done to examine the data drawn from the spoken corpora which are commonly used in the training and evaluation of ASRs in order to understand whether or not they are actually biased, this study seeks to begin addressing this gap in the research by investigating spoken corpora used for ASR training and evaluation for a grammatical linguistic feature of what the field of linguistics terms African American Language (AAL), a systematic, rule-governed, and legitimate linguistic variety spoken by many (but not all) African Americans in the U.S. This grammatical feature, habitual 'be', is an uninflected form of 'be' that encodes the characteristic of habituality, as in ""I be in my office by 7:30am"", paraphrasable as ""I am usually in my office by 7:30"" in Standardized American English. This study utilizes established corpus linguistics methods on the transcribed data of four major spoken corpora -- Switchboard, Fisher, TIMIT, and LibriSpeech -- to understand the frequency, distribution, and usage of habitual 'be' within each corpus as compared to a reference corpus of spoken AAL -- the Corpus of Regional African American Language (CORAAL). The results find that habitual 'be' appears far less frequently, is dispersed in far fewer transcribed texts, and is surrounded by a much less diverse set of word types and parts of speech in the four ASR corpora as compared with CORAAL. This work provides foundational evidence that spoken corpora used in the training and evaluation of widely used ASR systems are, in fact, biased against AAL and likely contribute to poorer ASR performance for Black users.","['automatic speech recognition', 'spoken corpora', 'datasets', 'linguistic bias', 'racial bias', 'African American Language']","['Social and professional topics _ Race and ethnicity', 'Human-centered computing _ Natural language interfaces']",['Joshua L Martin'],"['University of Florida Gainesville, Florida']",[None]
2021,https://dl.acm.org/doi/10.1145/3442188.3445907,Towards Cross-Lingual Generalization of Translation Gender Bias,Fairness & Bias,"Cross-lingual generalization issues for less explored languages have been broadly tackled in recent NLP studies. In this study, we apply the philosophy on the problem of translation gender bias, which necessarily involves multilingualism and socio-cultural diversity. Beyond the conventional evaluation criteria for the social bias, we aim to put together various aspects of linguistic viewpoints into the measuring process, to create a template that makes evaluation less tilted to specific types of language pairs. With a manually constructed set of content words and template, we check both the accuracy of gender inference and the fluency of translation, for German, Korean, Portuguese, and Tagalog. Inference accuracy and disparate impact, namely the biasedness factors associated with each other, show that the failure of bias mitigation threatens the delicacy of translation. Furthermore, our analyses on each system and language indicate that the translation fluency and inference accuracy are not necessarily correlated. The results implicitly suggest that the amount of available language resources that boost up the performance might amplify the bias cross-linguistically.","['machine translation', 'gender bias', 'evaluation', 'cross-linguality']","['Computing methodologies _ Machine translation', 'Model verification and validation']","['Won Ik Cho', 'Jiwon Kim', 'Jaeyeong Yang', 'Nam Soo Kim']","['Dept. of ECE and INMC, Seoul National University, Seoul', 'Independent Researcher, Daegu', 'Dept. of Linguistics, Seoul National University, Seoul', 'Dept. of ECE and INMC, Seoul National University, Seoul']","['Korea', 'Korea', 'Korea', 'Korea']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445894,Leave-one-out Unfairness,Fairness & Bias,"We introduce leave-one-out unfairness, which characterizes how likely a model's prediction for an individual will change due to the inclusion or removal of a single other person in the model's training data. Leave-one-out unfairness appeals to the idea that fair decisions are not arbitrary: they should not be based on the chance event of any one person's inclusion in the training data. Leave-one-out unfairness is closely related to algorithmic stability, but it focuses on the consistency of an individual point's prediction outcome over unit changes to the training data, rather than the error of the model in aggregate. Beyond formalizing leave-one-out unfairness, we characterize the extent to which deep models behave leave-one-out unfairly on real data, including in cases where the generalization error is small. Further, we demonstrate that adversarial training and randomized smoothing techniques have opposite effects on leave-one-out fairness, which sheds light on the relationships between robustness, memorization, individual fairness, and leave-one-out fairness in deep models. Finally, we discuss salient practical applications that may be negatively affected by leave-one-out unfairness.",[],[],"['Emily Black', 'Matt Fredrikson']","['Carnegie Mellon University', 'Carnegie Mellon University']","[None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445864,Price Discrimination with Fairness Constraints,Fairness & Bias,"Price discrimination - offering different prices to different customers - has become common practice. While it allows sellers to increase their profits, it also raises several concerns in terms of fairness. This topic has received extensive attention from media, industry, and regulatory agencies. In this paper, we consider the problem of setting prices for different groups under fairness constraints. In this paper, we propose a formal framework for pricing with fairness, including several definitions of fairness and their potential impact on consumers, sellers, and society at large. In a first step towards the ambitious agenda of designing pricing strategies that are fair, we consider the simplest scenario of a single-product seller facing consumers who can be partitioned into two groups based on a single, binary feature observable to the seller. For each group, we assume that the seller knows the valuation distribution and the population size. The seller's goal is to maximize profit by optimally selecting a price for each group, subject to a fairness constraint which may be self-imposed or explicitly enforced by laws and regulations. We first propose four definitions: fairness in price, demand, consumer surplus, and no-purchase valuation. With our model and definitions in place, we first show that satisfying all four fairness goals simultaneously is impossible unless the mean valuations are the same for both groups. In fact, even achieving two fairness measures simultaneously cannot be done in basic settings. We then consider the impact of imposing each fairness criterion separately, and identify conditions under which the consumer surplus and the social welfare increase or decrease. Under linear or exponential demand, we show that imposing a small amount of fairness in price or no-purchase valuation increases social welfare, whereas fairness in demand or surplus reduces social welfare. We fully characterize the impact of imposing different types of fairness for linear demand. We discover that imposing too much price fairness may result in a lower social welfare relative to imposing no price fairness. Imposing demand and surplus fairness always decreases social welfare. However, imposing no-purchase valuation fairness always increases social welfare. We also extend our results to the cases when there are multiple groups or there is an unprotected feature. Finally, we computationally show that most of our findings continue to hold for three common nonlinear demand models. Our results and insights provide a first step in understanding the impact of imposing fairness in the context of pricing.","['fairness', 'price discrimination', 'personalization', 'social welfare']","['Applied computing _ Economics; Law', 'Information systems _ Personalization', 'Social and professional topics _ Pricing and resource allocation']","['Maxime C. Cohen', 'Adam N. Elmachtoub', 'Xiao Lei']","['Desautels Faculty of Management, McGill University, Montreal, Quebec', 'Department of Industrial Engineering and Operations Research & Data Science Institute, Columbia University, New York, New York', 'Department of Industrial Engineering and Operations Research, Columbia University, New York, New York']","['Canada', 'USA', 'USA']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445869,Biases in Generative Art---A Causal Look from the Lens of Art History,Fairness & Bias,"With rapid progress in artificial intelligence (AI), popularity of generative art has grown substantially. From creating paintings to generating novel art styles, AI based generative art has showcased a variety of applications. However, there has been little focus concerning the ethical impacts of AI based generative art. In this work, we investigate biases in the generative art AI pipeline right from those that can originate due to improper problem formulation to those related to algorithm design. Viewing from the lens of art history, we discuss the socio-cultural impacts of these biases. Leveraging causal models, we highlight how current methods fall short in modeling the process of art creation and thus contribute to various types of biases. We illustrate the same through case studies, in particular those related to style transfer. To the best of our knowledge, this is the first extensive analysis that investigates biases in the generative art AI pipeline from the perspective of art history. We hope our work sparks interdisciplinary discussions related to accountability of generative art.","['generative art', 'style transfer', 'biases', 'AI', 'socio-cultural impacts']","[' Social and professional topics', 'Computing methodologies _ Artificial intelligence']","['Ramya Srinivasan', 'Kanji Uchino']","['Fujitsu Laboratories of America', 'Fujitsu Laboratories of America']","[None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445896,Re-imagining Algorithmic Fairness in India and Beyond,Fairness & Bias,"Conventional algorithmic fairness is West-centric, as seen in its subgroups, values, and methods. In this paper, we de-center algorithmic fairness and analyse AI power in India. Based on 36 qualitative interviews and a discourse analysis of algorithmic deployments in India, we find that several assumptions of algorithmic fairness are challenged. We find that in India, data is not always reliable due to socio-economic factors, ML makers appear to follow double standards, and AI evokes unquestioning aspiration. We contend that localising model fairness alone can be window dressing in India, where the distance between models and oppressed communities is large. Instead, we re-imagine algorithmic fairness in India and provide a roadmap to re-contextualise data and models, empower oppressed communities, and enable Fair-ML ecosystems.","['India', 'algorithmic fairness', 'caste', 'gender', 'religion', 'ability', 'class', 'feminism', 'decoloniality', 'anti-caste politics', 'critical algorithmic studies']",['Human-centered computing _ Empirical studies in HCI'],"['Nithya Sambasivan', 'Erin Arnesen', 'Ben Hutchinson', 'Tulsee Doshi', 'Vinodkumar Prabhakaran']","['Google Research Mountain View', 'Google Research Mountain View', 'Google Research Mountain View', 'Google Research Mountain View', 'Google Research Mountain View']","['CA', 'CA', 'CA', 'CA', 'CA']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445942,"The Algorithmic Leviathan: Arbitrariness, Unfairness, and Opportunity in Algorithmic Decision Making Systems",Fairness & Bias,"Automated decision-making systems implemented in public life are typically standardized. One algorithmic decision-making system can replace thousands of human deciders. Each of the humans so replaced had her own decision-making criteria: some good, some bad, and some arbitrary. Is such arbitrariness of moral concern? We argue that an isolated arbitrary decision need not morally wrong the individual whom it misclassifies. However, if the same algorithms are applied across a public sphere, such as hiring or lending, a person could be excluded from a large number of opportunities. This harm persists even when the automated decision-making systems are ""fair"" on standard metrics of fairness. We argue that such arbitrariness at scale is morally problematic and propose technically informed solutions that can lessen the impact of algorithms at scale and so mitigate or avoid the moral harms we identify.","['opportunity', 'arbitrariness', 'fairness', 'machine learning', 'algorithmic decision making', 'automated hiring']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Law', 'Computing methodologies _ Artificial intelligence']","['Kathleen Creel', 'Deborah Hellman']","['Stanford University, Palo Alto, California', 'University of Virginia, Charlottesville, Virginia']","['USA', 'USA']"
2021,https://dl.acm.org/doi/10.1145/3442188.3445876,Group Fairness: Independence Revisited,Fairness & Bias,"This paper critically examines arguments against independence, a measure of group fairness also known as statistical parity and as demographic parity. In recent discussions of fairness in computer science, some have maintained that independence is not a suitable measure of group fairness. This position is at least partially based on two influential papers (Dwork et al., 2012, Hardt et al., 2016) that provide arguments against independence. We revisit these arguments, and we find that the case against independence is rather weak. We also give arguments in favor of independence, showing that it plays a distinctive role in considerations of fairness. Finally, we discuss how to balance different fairness considerations.","['fairness', 'independence', 'statistical parity', 'demographic parity', 'sufficiency', 'separation', 'affirmative action', 'accuracy']","['Social and professional topics _ User characteristics', 'Computing methodologies _ Machine learning', 'Applied computing _ Arts and humanities']",['Tim Räz'],"['Institute of Philosophy, University of Bern, Switzerland Institute of Biomedical Ethics and History of Medicine, University of Zürich']",['Switzerland']
2021,https://dl.acm.org/doi/10.1145/3442188.3445886,The Use and Misuse of Counterfactuals in Fair Machine Learning,Fairness & Bias,"The use of counterfactuals for considerations of algorithmic fairness and explainability is gaining prominence within the machine learning community and industry. This paper argues for more caution with the use of counterfactuals when the facts to be considered are social categories such as race or gender. We review a broad body of papers from philosophy and social sciences on social ontology and the semantics of counterfactuals, and we conclude that the counterfactual approach in machine learning fairness and social explainability can require an incoherent theory of what social categories are. Our findings suggest that most often the social categories may not admit counterfactual manipulation, and hence may not appropriately satisfy the demands for evaluating the truth or falsity of counterfactuals. This is important because the widespread use of counterfactuals in machine learning can lead to misleading results when applied in high-stakes domains. Accordingly, we argue that even though counterfactuals play an essential part in some causal inferences, their use for questions of algorithmic fairness and social explanations can create more problems than they resolve. Our positive result is a set of tenets about using counterfactuals for fairness and explanations in machine learning.","['Ethics of AI', 'Ethical AI', 'Counterfactuals', 'Machine learning', 'Fairness', 'Algorithmic Fairness', 'Explanation', 'Explainable AI', 'Philosophy', 'Social ontology', 'Social category', 'Social kind', 'Philosophy of AI']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Machine learning', 'Social and professional topics _ Socio-technical systems', 'Race and ethnicity']","['Atoosa Kasirzadeh', 'Andrew Smart']","['University of Toronto, Australian National University', 'Google']","[None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445930,The Effect of the Rooney Rule on Implicit Bias in the Long Term,Fairness & Bias,"The Rooney Rule, originally proposed to counter implicit bias in hiring, has been implemented in the private and public sector in various settings. This rule requires that a decision-maker include at least one candidate from an underrepresented group in their shortlist of candidates. Recently, [42] proposed a mathematical model of implicit bias and studied the effectiveness of the Rooney Rule when applied to a single selection decision. However, selection decisions often occur repeatedly over time; e.g., a software firm is continuously hiring employees or a university makes admissions decisions every year. Further, it has been observed that, given consistent counterstereotypical feedback, implicit biases against underrepresented candidates can change. In this paper, we propose a model of how a decision-maker's implicit bias changes over time given their hiring decisions either with or without the Rooney Rule in place. Our model draws from the work of [42] and the literature on opinion dynamics. Our main result is that, for this model, when the decision-maker is constrained by the Rooney Rule, their implicit bias roughly reduces at a rate that is inverse of the size of the shortlist---independent of the total number of candidates, whereas without the Rooney Rule, the rate is inversely proportional to the number of candidates. Thus, our model predicts that when the number of candidates is much larger than the size of the shortlist, the Rooney Rule enables a significantly faster reduction in implicit bias, providing additional reason in favor of instating it as a strategy to mitigate implicit bias. Towards empirically evaluating the long-term effect of the Rooney Rule in repeated selection decisions, we conduct an iterative candidate selection experiment on Amazon Mechanical Turk. We observe that, indeed, decision-makers subject to the Rooney Rule select more minority candidates in addition to those required by the rule itself than they would if no rule is in effect, and in fact are able to do so without considerably decreasing the utility of candidates selected.",[],[],"['L. Elisa Celis', 'Chris Hays', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'Yale University', 'Yale University', 'Yale University']","[None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445873,"The Distributive Effects of Risk Prediction in Environmental Compliance: Algorithmic Design, Environmental Justice, and Public Policy",Fairness & Bias,"Government agencies are embracing machine learning to support a variety of resource allocation decisions. The U.S. Environmental Protection Agency (EPA), for example, has engaged academic research labs to test the use of machine learning in support of an important national initiative to reduce Clean Water Act violations. We evaluate prototypical risk prediction models that can support compliance interventions and demonstrate how critical algorithmic design choices can generate or mitigate disparate impact in environmental enforcement. First, we show that the definition of which facilities to focus on through this national compliance initiative hinges on arbitrary differences in state-level permitting schemes, causing a shift in environmental protection away from areas with more minority populations. Second, the policy objective to reduce the noncompliance rate is encoded in a classification model, which does not account for the extent of pollution beyond the permitted limit. We hence compare allocation schemes between regression and classification, and show that the latter directs attention towards facilities in more rural and white areas. Overall, our study illustrates that as machine learning enters government, algorithmic design can both embed and elucidate sources of administrative policy discretion with discernable distributional consequences.","['risk models', 'government', 'environmental protection', 'fairness', 'environmental justice']","['Social and professional topics _ Governmental regulations', 'Applied computing _ Law', 'Computing in government', 'Computing methodologies _ Machine learning', 'Human-centered computing _ Interaction design']","['Elinor Benami', 'Reid Whitaker', 'Vincent La', 'Hongjin Lin', 'Brandon R. Anderson', 'Daniel E. Ho']","['Virginia Tech', 'University of California Berkeley', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","[None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445938,An Action-Oriented AI Policy Toolkit for Technology Audits by Community Advocates and Activists,Security,"Motivated by the extensive documented disparate harms of artificial intelligence (AI), many recent practitioner-facing reflective tools have been created to promote responsible AI development. However, the use of such tools internally by technology development firms addresses responsible AI as an issue of closed-door compliance rather than a matter of public concern. Recent advocate and activist efforts intervene in AI as a public policy problem, inciting a growing number of cities to pass bans or other ordinances on AI and surveillance technologies. In support of this broader ecology of political actors, we present a set of reflective tools intended to increase public participation in technology advocacy for AI policy action. To this end, the Algorithmic Equity Toolkit (the AEKit) provides a practical policy-facing definition of AI, a flowchart for assessing technologies against that definition, a worksheet for decomposing AI systems into constituent parts, and a list of probing questions that can be posed to vendors, policy-makers, or government agencies. The AEKit carries an action-orientation towards political encounters between community groups in the public and their representatives, opening up the work of AI reflection and remediation to multiple points of intervention. Unlike current reflective tools available to practitioners, our toolkit carries with it a politics of community participation and activism.","['Participatory design', 'participatory action research', 'accountability', 'algorithmic equity', 'algorithmic justice', 'surveillance', 'regulation']","['Social and professional topics _ Surveillance', 'Governmental regulations', 'Computing literacy', 'Human-centered computing _ Participatory design', 'Computing methodologies _ Artificial intelligence']","['P. M. Krafft', 'Meg Young', 'Michael Katell', 'Jennifer E. Lee', 'Shankar Narayan', 'Micah Epstein', 'Dharma Dailey', 'Bernease Herman', 'Aaron Tam', 'Vivian Guetler', 'Corinne Bintz', 'Daniella Raz', 'Pa Ousman Jobe', 'Franziska Putz', 'Brian Robick', 'Bissan Barghouti']","['Creative Computing Institute, University of Arts London', 'Digital Life Initiative, Cornell Tech', 'Public Policy Programme, Alan Turing Institute', 'ACLU of Washington', 'MIRA', 'Coveillance Collective', 'Human Centered Design & Engineering, University of Washington', 'eScience Institute, University of Washington', 'Evans School of Public Policy & Governance, University of Washington', 'Department of Sociology, West Virginia University', 'Department of Computer Science, Middlebury College', 'School of Information, University of Michigan', 'Albers School of Business & Economics, Seattle University', 'Oxford Department of International Development, University of Oxford', 'ACLU of Washington', 'ACLU of Washington']","[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]"
2021,https://dl.acm.org/doi/10.1145/3442188.3445881,Leveraging Administrative Data for Bias Audits: Assessing Disparate Coverage with Mobility Data for COVID-19 Policy,Security,"Anonymized smartphone-based mobility data has been widely adopted in devising and evaluating COVID-19 response strategies such as the targeting of public health resources. Yet little attention has been paid to measurement validity and demographic bias, due in part to the lack of documentation about which users are represented as well as the challenge of obtaining ground truth data on unique visits and demographics. We illustrate how linking large-scale administrative data can enable auditing mobility data for bias in the absence of demographic information and ground truth labels. More precisely, we show that linking voter roll data---containing individual-level voter turnout for specific voting locations along with race and age---can facilitate the construction of rigorous bias and reliability tests. Using data from North Carolina's 2018 general election, these tests illuminate a sampling bias that is particularly noteworthy in the pandemic context: older and non-white voters are less likely to be captured by mobility data. We show that allocating public health resources based on such mobility data could disproportionately harm high-risk elderly and minority groups.",[],[],"['Amanda Coston', 'Neel Guha', 'Derek Ouyang', 'Lisa Lu', 'Alexandra Chouldechova', 'Daniel E. Ho']","['Carnegie Mellon University', 'Stanford University', 'Stanford University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University']","[None, None, None, None, None, None]"