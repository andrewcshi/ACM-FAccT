doi_link,title,category,abstract,keywords,CCS concepts,author_names,affiliations,affiliated_countries
https://dl.acm.org/doi/abs/10.1145/3351095.3372873,Closing the AI Accountability Gap: Defining an End-to-End Framework for Internal Algorithmic Auditing,Transparency & Explainability,"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.","['Algorithmic audits', 'machine learning', 'accountability', 'responsible innovation']","['Social and professional topics _ System management', 'Tech-nology audits', 'Software and its engineering _ Software de-velopment process management']","['Inioluwa Deborah Raji', 'Andrew Smart', 'Rebecca N. White', 'Margaret Mitchell', 'Timnit Gebru', 'Ben Hutchinson', 'Jamila Smith-Loud', 'Daniel Theron', 'Parker Barnes']","['Partnership on AI', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","[None, None, None, None, None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372870,Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches,Transparency & Explainability,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","['Explainability', 'Interpretability', 'Transparency', 'Fact Sheet', 'Work Sheet', 'Desiderata', 'Taxonomy', 'AI', 'ML']","['General and reference _ Evaluation', 'Computing method-ologies _ Artificial intelligence', 'Machine learning']","['Kacper Sokol', 'Peter Flach']","['University of Bristol, Bristol', 'University of Bristol, Bristol']","['United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372879,Auditing Radicalization Pathways on YouTube,Transparency & Explainability,"Non-profits, as well as the media, have hypothesized the existence of a radicalization pipeline on YouTube, claiming that users systematically progress towards more extreme content on the platform. Yet, there is to date no substantial quantitative evidence of this alleged pipeline. To close this gap, we conduct a large-scale audit of user radicalization on YouTube. We analyze 330,925 videos posted on 349 channels, which we broadly classified into four types: Media, the Alt-lite, the Intellectual Dark Web (I.D.W.), and the Alt-right. According to the aforementioned radicalization hypothesis, channels in the I.D.W. and the Alt-lite serve as gateways to fringe far-right ideology, here represented by Alt-right channels. Processing 72M+ comments, we show that the three channel types indeed increasingly share the same user base; that users consistently migrate from milder to more extreme content; and that a large percentage of users who consume Alt-right content now consumed Alt-lite and I.D.W. content in the past. We also probe YouTube's recommendation algorithm, looking at more than 2M video and channel recommendations between May/July 2019. We find that Alt-lite content is easily reachable from I.D.W. channels, while Alt-right videos are reachable only through channel recommendations. Overall, we paint a comprehensive picture of user radicalization on YouTube.","['Radicalization', 'hate speech', 'extremism', 'algorithmic auditing']",['Human-centered computing _ Empirical studies in collab-orative and social computing'],"['Manoel Horta Ribeiro', 'Raphael Ottoni', 'Robert West', 'Virg√≠lio A. F. Almeida', 'Wagner Meira']","['EPFL', 'UFMG', 'EPFL', 'UFMG', 'UFMG']","[None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372856,"Regulating Transparency? Facebook, Twitter and the German Network Enforcement Act",Transparency & Explainability,"Regulatory regimes designed to ensure transparency often struggle to ensure that transparency is meaningful in practice. This challenge is particularly great when coupled with the widespread usage of dark patterns --- design techniques used to manipulate users. The following article analyses the implementation of the transparency provisions of the German Network Enforcement Act (NetzDG) by Facebook and Twitter, as well as the consequences of these implementations for the effective regulation of online platforms. This question of effective regulation is particularly salient, due to an enforcement action in 2019 by Germany's Federal Office of Justice (BfJ) against Facebook for what the BfJ claim were insufficient compliance with transparency requirements, under NetzDG. This article provides an overview of the transparency requirements of NetzDG and contrasts these with the transparency requirements of other relevant regulations. It will then discuss how transparency concerns not only providing data, but also how the visibility of the data that is made transparent is managed, by deciding how the data is provided and is framed. We will then provide an empirical analysis of the design choices made by Facebook and Twitter, to assess the ways in which their implementations differ. The consequences of these two divergent implementations on interface design and user behaviour are then discussed, through a comparison of the transparency reports and reporting mechanisms used by Facebook and Twitter. As a next step, we will discuss the BfJ's consideration of the design of Facebook's content reporting mechanisms, and what this reveals about their respective interpretations of NetzDG's scope. Finally, in recognising that this situation is one in which a regulator is considering design as part of their action - we develop a wider argument on the potential for regulatory enforcement around dark patterns, and design practices more generally, for which this case is an early, indicative example.",[],[],"['Ben Wagner', 'Krisztina Rozgonyi', 'Marie-Therese Sekwenz', 'Jennifer Cobbe', 'Jatinder Singh']","['Vienna University of Economics and Business', 'University of Vienna', 'Vienna University of Economics and Business', 'University of Cambridge', 'University of Cambridge']","[None, None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372835,The Case for Voter-Centered Audits of Search Engines during Political Elections,Transparency & Explainability,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['algorithm audits', 'search engines', 'Google', 'voters', 'elections', 'bias']",['Information systems _ Web search engines'],"['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372841,Whose Tweets are Surveilled for the Police: An Audit of a Social-Media Monitoring Tool via Log Files,Transparency & Explainability,"Social media monitoring by law enforcement is becoming commonplace, but little is known about what software packages for it do. Through public records requests, we obtained log files from the Corvallis (Oregon) Police Department's use of social media monitoring software called DigitalStakeout. These log files include the results of proprietary searches by DigitalStakeout that were running over a period of 13 months and include 7240 social media posts. In this paper, we focus on the Tweets logged in this data and consider the racial and ethnic identity (through manual coding) of the users that are therein flagged by DigitalStakeout. We observe differences in the demographics of the users whose Tweets are flagged by DigitalStakeout compared to the demographics of the Twitter users in the region, however, our sample size is too small to determine significance. Further, the demographics of the Twitter users in the region do not seem to reflect that of the residents of the region, with an apparent higher representation of Black and Hispanic people. We also reconstruct the keywords related to a Narcotics report set up by DigitalStakeout for the Corvallis Police Department and find that these keywords flag Tweets unrelated to narcotics or flag Tweets related to marijuana, a drug that is legal for recreational use in Oregon. Almost all of the keywords have a common meaning unrelated to narcotics (e.g. broken, snow, hop, high) that call into question the utility that such a keyword based search could have to law enforcement. As social media monitoring is increasingly used for law enforcement purposes, racial biases in surveillance may contribute to existing racial disparities in law enforcement practices. We are hopeful that log files obtainable through public records request will shed light on the operation of these surveillance tools. There are challenges in auditing these tools: public records requests may go unfulfilled even if the data is available, social media platforms may not provide comparable data for comparison with surveillance data, demographics can be difficult to ascertain from social media and Institutional Review Boards may not understand how to weigh the ethical considerations involved in this type of research. We include in this paper a discussion of our experience in navigating these issues.","['social media monitoring', 'surveillance', 'police', 'demographics', 'key-words', 'audit']","['Social and professional topics _ Governmental surveil-lance', 'Corporate surveillance', 'Race and ethnicity', 'General and reference _ Empirical studies']","['Glencora Borradaile', 'Brett Burkhardt', 'Alexandria LeClerc']","['Oregon State University', 'Oregon State University', 'Oregon State University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372825,Dirichlet uncertainty wrappers for actionable algorithm accuracy accountability and auditability,Transparency & Explainability,"Nowadays, the use of machine learning models is becoming a utility in many applications. Companies deliver pre-trained models encapsulated as application programming interfaces (APIs) that developers combine with third-party components and their own models and data to create complex data products to solve specific problems. The complexity of such products and the lack of control and knowledge of the internals of each component used unavoidable cause effects, such as lack of transparency, difficulty in auditability, and the emergence of potential uncontrolled risks. They are effectively black-boxes. Accountability of such solutions is a challenge for the auditors and the machine learning community. In this work, we propose a wrapper that given a black-box model enriches its output prediction with a measure of uncertainty when applied to a target domain. To develop the wrapper, we follow these steps: Modeling the distribution of the output. In a text classification setting, the output is a probability distribution p(y|X, w*) over the different classes to predict, y, given an input text X and the pre-trained model with parameters w*. We model this output by a random variable to measure the variability that the data noise causes in the output. Here we consider the output distribution coming from a Dirichlet probability density function, thus p(y|X, w*) ~ Dir(Œ±). Decomposition of the Dirichlet concentration parameter. To relate the output of the classifier with the concentration parameter in the Dirichlet distribution, we propose a decomposition of the concentration parameter in two terms: Œ± = Œ≤y. The role of this scalar Œ≤ is to control the spread of the distribution around the expected value, i.e. the original prediction y. Training the wrapper. Sentences are represented as the average value of their word embeddings. This representation feeds a neural network that outputs a single regression value that models the parameter Œ≤. For each input, we combine Œ≤ and the black-box prediction to obtain the corresponding distribution for the output ym,i ~ Dir(Œ±i). By using Monte Carlo sampling, we approximate the expected value of the classification probabilities, [EQUATION] and we train the model applying a cross-entropy loss over the predictions and the labels. Obtaining an uncertainty score from the wrapper. To obtain a numerical value for the uncertainty of a prediction, we draw samples from the resulting Dir(Œ±) to evaluate the predictive entropy with [EQUATION], thus obtaining a numerical score for the uncertainty of each prediction. Using uncertainty for rejection. Based on this wrapper, we provide an actionable mechanism to mitigate risk in the form of decision rejection: once equipped with a value for the uncertainty of a given prediction, we can choose not to issue that prediction when the risk or uncertainty in that decision is significant. This results in a rejection system that selects the more confident predictions, discards those more uncertain, and leads to an improvement in the trustability of the resulting system. We showcase the proposed technique and methodology in a practical scenario where we apply a simulated sentiment analysis API based on NLP to different domains. On each experiment, we train a sentiment classifier using text reviews of products in a source domain. We apply the pre-trained black-box to obtain the predictions for the reviews from a target domain. The tuples of review plus black-box predictions are then used for training the wrapper to obtain the uncertainty. Finally, we use the uncertainty score to sort the predictions from more to less uncertain, and we search for a rejection point that maximizes the three performance measures: non-rejected accuracy, and classification and rejection quality. Experiments demonstrate the effectiveness of the uncertainty measure computed by the wrapper and shows its high correlation to bad quality predictions and misclassifications. In all the cases, the uncertainty metric here proposed outperforms traditional uncertainty measures.","['auditability', 'accuracy', 'uncertainty', 'black-box models', 'machine learn-ing']",[],"['Jos√© Mena Roldán', 'Oriol Pujol Vila', 'Jordi Vitri√† Marca']","['Universitat de Barcelona', 'Universitat de Barcelona', 'Universitat de Barcelona']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375234,Model Agnostic Interpretability of Text Rankers via Intent Modelling,Transparency & Explainability,"A key problem in information retrieval is understanding the latent intention of a user's under-specified query. Retrieval models that are able to correctly uncover the query intent often perform well on the document ranking task. In this paper we study the problem of interpretability for text based ranking models by trying to unearth the query intent as understood by complex retrieval models. We propose a model-agnostic approach that attempts to locally approximate a complex ranker by using a simple ranking model in the term space. Given a query and a blackbox ranking model, we propose an approach that systematically exploits preference pairs extracted from the target ranking and document perturbations to identify a set of intent terms and a simple term based ranker that can faithfully and accurately mimic the complex blackbox ranker in that locality. Our results indicate that we can indeed interpret more complex models with high fidelity. We also present a case study on how our approach can be used to interpret recently proposed neural rankers.",[],[],"['Jaspreet Singh', 'Avishek Anand']","['L3S Research Center, Hannover', 'L3S Research Center, Hannover']","['Germany', 'Germany']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372855,Doctor XAI: An ontology-based approach to black-box sequential data classification explanations,Transparency & Explainability,"Several recent advancements in Machine Learning involve blackbox models: algorithms that do not provide human-understandable explanations in support of their decisions. This limitation hampers the fairness, accountability and transparency of these models; the field of eXplainable Artificial Intelligence (XAI) tries to solve this problem providing human-understandable explanations for black-box models. However, healthcare datasets (and the related learning tasks) often present peculiar features, such as sequential data, multi-label predictions, and links to structured background knowledge. In this paper, we introduce Doctor XAI, a model-agnostic explainability technique able to deal with multi-labeled, sequential, ontology-linked data. We focus on explaining Doctor AI, a multilabel classifier which takes as input the clinical history of a patient in order to predict the next visit. Furthermore, we show how exploiting the temporal dimension in the data and the domain knowledge encoded in the medical ontology improves the quality of the mined explanations.","['explainable artificial intelligence', 'machine learning', 'healthcare data']","['Computing methodologies _ Artificial intelligence', 'Ma-chine learning', 'Applied computing _ Health care infor-mation systems']","['Cecilia Panigutti', 'Alan Perotti', 'Dino Pedreschi']","['Scuola Normale Superiore', 'ISI foundation', 'University of Pisa']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3373154,Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination,Fairness & Bias,"The increasing impact of algorithmic decisions on people's lives compels us to scrutinize their fairness and, in particular, the disparate impacts that ostensibly-color-blind algorithms can have on different groups. Examples include credit decisioning, hiring, advertising, criminal justice, personalized medicine, and targeted policymaking, where in some cases legislative or regulatory frameworks for fairness exist and define specific protected classes. In this paper we study a fundamental challenge to assessing disparate impacts, or performance disparities in general, in practice: protected class membership is often not observed in the data. This is particularly a problem in lending and healthcare. We consider the use of an auxiliary dataset, such as the US census, that includes protected class labels but not decisions or outcomes. We show that a variety of common disparity measures are generally unidentifiable aside for some unrealistic cases, providing a new perspective on the documented biases of popular proxy-based methods. We provide exact characterizations of the sharpest-possible partial identification set of disparities either under no assumptions or when we incorporate mild smoothness constraints. We further provide optimization-based algorithms for computing and visualizing these sets of simultaneously achievable pairwise disparties for assessing disparities that arise between multiple groups, which enables reliable and robust assessments - an important tool when disparity assessment can have far-reaching policy implications. We demonstrate this in two case studies with real data: mortgage lending and personalized medicine dosing.",[],[],"['Nathan Kallus', 'Xiaojie Mao', 'Angela Zhou']","['Cornell University', 'Cornell University', 'Cornell University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372859,Studying Up: Reorienting the study of algorithmic fairness around issues of power,Fairness & Bias,"Research within the social sciences and humanities has long characterized the work of data science as a sociotechnical process, comprised of a set of logics and techniques that are inseparable from specific social norms, expectations and contexts of development and use. Yet all too often the assumptions and premises underlying data analysis remain unexamined, even in contemporary debates about the fairness of algorithmic systems. This blindspot exists in part because the methodological toolkit used to evaluate the fairness of algorithmic systems remains limited to a narrow set of computational and legal modes of analysis. In this paper, we expand on Elish and Boyd's [17] call for data scientists to develop more robust frameworks for understanding their work as situated practice by examining a specific methodological debate within the field of anthropology, frequently referred to as the practice of ""studying up"". We reflect on the contributions that the call to ""study up"" has made in the field of anthropology before making the case that the field of algorithmic fairness would similarly benefit from a reorientation ""upward"". A case study from our own work illustrates what it looks like to reorient one's research questions ""up"" in a high-profile debate regarding the fairness of an algorithmic system - namely, pretrial risk assessment in American criminal law. We discuss the limitations of contemporary fairness discourse with regard to pretrial risk assessment before highlighting the insights gained when we reframe our research questions to focus on those who inhabit positions of power and authority within the U.S. court system. Finally, we reflect on the challenges we have encountered in implementing data science projects that ""study up"". In the process, we surface new insights and questions about what it means to ethically engage in data science work that directly confronts issues of power and authority.",[],[],"['Chelsea Barabas', 'Colin Doyle', 'JB Rubinovitz', 'Karthik Dinakar']","['Massachusetts Institute of Technology', 'Harvard Law School', 'Massachusetts Institute of Technology', 'Harvard Law School']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375783,Bidding Strategies with Gender Nondiscrimination Constraints for Online Ad Auctions,Fairness & Bias,"Interactions between bids to show ads online can lead to an advertiser's ad being shown to more men than women even when the advertiser does not target towards men. We design bidding strategies that advertisers can use to avoid such emergent discrimination without having to modify the auction mechanism. We mathematically analyze the strategies to determine the additional cost to the advertiser for avoiding discrimination, proving our strategies to be optimal in some settings. We use simulations to understand other settings.","['targeted advertising', 'online auctions', 'fairness constraints', 'MDPs']",['Information systems _ Online advertising'],"['Milad Nasr', 'Michael Carl Tschantz']","['University of Massachusetts Amherst', 'International Computer Science Institute']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372858,Interventions for Ranking in the Presence of Implicit Bias,Fairness & Bias,"Implicit bias is the unconscious attribution of particular qualities (or lack thereof) to a member from a particular social group (e.g., defined by gender or race). Studies on implicit bias have shown that these unconscious stereotypes can have adverse outcomes in various social contexts, such as job screening, teaching, or policing. Recently, [34] considered a mathematical model for implicit bias and showed the effectiveness of the Rooney Rule as a constraint to improve the utility of the outcome for certain cases of the subset selection problem. Here we study the problem of designing interventions for the generalization of subset selection - ranking - that requires to output an ordered set and is a central primitive in various social and computational contexts. We present a family of simple and interpretable constraints and show that they can optimally mitigate implicit bias for a generalization of the model studied in [34]. Subsequently, we prove that under natural distributional assumptions on the utilities of items, simple, Rooney Rule-like, constraints can also surprisingly recover almost all the utility lost due to implicit biases. Finally, we augment our theoretical results with empirical findings on real-world distributions from the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.","['Implicit bias', 'ranking', 'algorithmic fairness', 'interventions']","['Information systems _ Content ranking', 'Mathematics ofcomputing _ Probability and statistics']","['L. Elisa Celis', 'Anay Mehrotra', 'Nisheeth K. Vishnoi']","['Yale University', 'IIT Kanpur', 'Yale University']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375623,Artificial mental phenomena: Psychophysics as a framework to detect perception biases in AI models,Fairness & Bias,"Detecting biases in artificial intelligence has become difficult because of the impenetrable nature of deep learning. The central difficulty is in relating unobservable phenomena deep inside models with observable, outside quantities that we can measure from inputs and outputs. For example, can we detect gendered perceptions of occupations (e.g., female librarian, male electrician) using questions to and answers from a word embedding-based system? Current techniques for detecting biases are often customized for a task, dataset, or method, affecting their generalization. In this work, we draw from Psychophysics in Experimental Psychology---meant to relate quantities from the real world (i.e., ""Physics"") into subjective measures in the mind (i.e., ""Psyche"")---to propose an intellectually coherent and generalizable framework to detect biases in AI. Specifically, we adapt the two-alternative forced choice task (2AFC) to estimate potential biases and the strength of those biases in black-box models. We successfully reproduce previously-known biased perceptions in word embeddings and sentiment analysis predictions. We discuss how concepts in experimental psychology can be naturally applied to understanding artificial mental phenomena, and how psychophysics can form a useful methodological foundation to study fairness in AI.","['Biases in word embeddings', 'Biases in Sentiment Analysis', 'Artificial Psychophysics', 'Two-alternative forced choice task']",[],"['Lizhen Liang', 'Daniel E. Acuna']","['Syracuse University', 'Syracuse University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372843,Bias in word embeddings,Fairness & Bias,"Word embeddings are a widely used set of natural language processing techniques that map words to vectors of real numbers. These vectors are used to improve the quality of generative and predictive models. Recent studies demonstrate that word embeddings contain and amplify biases present in data, such as stereotypes and prejudice. In this study, we provide a complete overview of bias in word embeddings. We develop a new technique for bias detection for gendered languages and use it to compare bias in embeddings trained on Wikipedia and on political social media data. We investigate bias diffusion and prove that existing biases are transferred to further machine learning models. We test two techniques for bias mitigation and show that the generally proposed methodology for debiasing models at the embeddings level is insufficient. Finally, we employ biased word embeddings and illustrate that they can be used for the detection of similar biases in new data. Given that word embeddings are widely used by commercial companies, we discuss the challenges and required actions towards fair algorithmic implementations and applications.","['word embeddings', 'bias', 'detection', 'diffusion', 'mitigation', 'fairness', 'sexism', 'racism', 'homophobia']","['Human-centered computing _ HCI design and evaluation methods', 'Computing methodologies _ Machine learning', 'Information systems _ Data mining']","['Orestis Papakyriakopoulos', 'Simon Hegelich', 'Juan Carlos Medina Serrano', 'Fabienne Marco']","['Technical University of Munich, Munich', 'Technical University of Munich, Munich', 'Technical University of Munich, Munich', 'Technical University of Munich, Munich']","['Germany', 'Germany', 'Germany', 'Germany']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372849,"What does it mean to 'solve' the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems",Fairness & Bias,"Discriminatory practices in recruitment and hiring are an ongoing issue that is a concern not just for workplace relations, but also for wider understandings of economic justice and inequality. The ability to get and keep a job is a key aspect of participating in society and sustaining livelihoods. Yet the way decisions are made on who is eligible for jobs, and why, are rapidly changing with the advent and growth in uptake of automated hiring systems (AHSs) powered by data-driven tools. Evidence of the extent of this uptake around the globe is scarce, but a recent report estimated that 98% of Fortune 500 companies use Applicant Tracking Systems of some kind in their hiring process, a trend driven by perceived efficiency measures and cost-savings. Key concerns about such AHSs include the lack of transparency and potential limitation of access to jobs for specific profiles. In relation to the latter, however, several of these AHSs claim to detect and mitigate discriminatory practices against protected groups and promote diversity and inclusion at work. Yet whilst these tools have a growing user-base around the world, such claims of 'bias mitigation' are rarely scrutinised and evaluated, and when done so, have almost exclusively been from a US socio-legal perspective. In this paper, we introduce a perspective outside the US by critically examining how three prominent automated hiring systems (AHSs) in regular use in the UK, HireVue, Pymetrics and Applied, understand and attempt to mitigate bias and discrimination. These systems have been chosen as they explicitly claim to address issues of discrimination in hiring and, unlike many of their competitors, provide some information about how their systems work that can inform an analysis. Using publicly available documents, we describe how their tools are designed, validated and audited for bias, highlighting assumptions and limitations, before situating these in the socio-legal context of the UK. The UK has a very different legal background to the US in terms not only of hiring and equality law, but also in terms of data protection (DP) law. We argue that this might be important for addressing concerns about transparency and could mean a challenge to building bias mitigation into AHSs definitively capable of meeting EU legal standards. This is significant as these AHSs, especially those developed in the US, may obscure rather than improve systemic discrimination in the workplace.","['Socio-technical systems', 'automated hiring', 'algorithmic decision-making', 'fairness', 'discrimination', 'GDPR', 'social justice']","['Social and professional topics _ Socio-technical systems', 'Systems analysis and design', 'Applied computing _ Law', 'Soci-ology']","['Javier Sánchez-Monedero', 'Lina Dencik', 'Lilian Edwards']","['Cardiff University, Cardiff, Wales', 'Cardiff University, Cardiff, Wales', 'University of Newcastle, Newcastle upon Tyne, England']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://dl.acm.org/doi/abs/10.1145/3351095.3372828,Mitigating Bias in Algorithmic Hiring: Evaluating Claims and Practices,Fairness & Bias,"There has been rapidly growing interest in the use of algorithms in hiring, especially as a means to address or mitigate bias. Yet, to date, little is known about how these methods are used in practice. How are algorithmic assessments built, validated, and examined for bias? In this work, we document and analyze the claims and practices of companies offering algorithms for employment assessment. In particular, we identify vendors of algorithmic pre-employment assessments (i.e., algorithms to screen candidates), document what they have disclosed about their development and validation procedures, and evaluate their practices, focusing particularly on efforts to detect and mitigate bias. Our analysis considers both technical and legal perspectives. Technically, we consider the various choices vendors make regarding data collection and prediction targets, and explore the risks and trade-offs that these choices pose. We also discuss how algorithmic de-biasing techniques interface with, and create challenges for, antidiscrimination law.","['algorithmic hiring', 'discrimination law', 'algorithmic bias']","['Social and professional topics _ Employment issues', 'Com-puting methodologies _ Machine learning', 'Applied comput-ing _ Law']","['Manish Raghavan', 'Solon Barocas', 'Jon Kleinberg', 'Karen Levy']","['Cornell University', 'Microsoft Research and Cornell University', 'Cornell University', 'Cornell University']","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372877,Awareness in Practice: Tensions in Access to Sensitive Attribute Data for Antidiscrimination,Fairness & Bias,"Organizations cannot address demographic disparities that they cannot see. Recent research on machine learning and fairness has emphasized that awareness of sensitive attributes, such as race and sex, is critical to the development of interventions. However, on the ground, the existence of these data cannot be taken for granted. This paper uses the domains of employment, credit, and healthcare in the United States to surface conditions that have shaped the availability of sensitive attribute data. For each domain, we describe how and when private companies collect or infer sensitive attribute data for antidiscrimination purposes. An inconsistent story emerges: Some companies are required by law to collect sensitive attribute data, while others are prohibited from doing so. Still others, in the absence of legal mandates, have determined that collection and imputation of these data are appropriate to address disparities. This story has important implications for fairness research and its future applications. If companies that mediate access to life opportunities are unable or hesitant to collect or infer sensitive attribute data, then proposed techniques to detect and mitigate bias in machine learning models might never be implemented outside the lab. We conclude that today's legal requirements and corporate practices, while highly inconsistent across domains, offer lessons for how to approach the collection and inference of sensitive data in appropriate circumstances. We urge stakeholders, including machine learning practitioners, to actively help chart a path forward that takes both policy goals and technical needs into account.",[],[],"['Miranda Bogen', 'Aaron Rieke', 'Shazeda Ahmed']","['Upturn', 'Upturn', 'University of California']","[None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372826,Towards a Critical Race Methodology in Algorithmic Fairness,Fairness & Bias,"We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems.","['algorithmic fairness', 'critical race theory', 'race and ethnicity']","['Applied computing _ Sociology', 'Social and professional topics _ Race and ethnicity']","['Alex Hanna', 'Emily Denton', 'Andrew Smart', 'Jamila Smith-Loud']","[None, None, None, None]","[None, None, None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3375674,What's Sex Got to Do With Fair Machine Learning?,Fairness & Bias,"The debate about fairness in machine learning has largely centered around competing substantive definitions of what fairness or nondiscrimination between groups requires. However, very little attention has been paid to what precisely a group is. Many recent approaches have abandoned observational, or purely statistical, definitions of fairness in favor of definitions that require one to specify a causal model of the data generating process. The implicit ontological assumption of these exercises is that a racial or sex group is a collection of individuals who share a trait or attribute, for example: the group ""female"" simply consists in grouping individuals who share female-coded sex features. We show this by exploring the formal assumption of modularity in causal models using directed acyclic graphs (DAGs), which hold that the dependencies captured by one causal pathway are invariant to interventions on any other causal pathways. Modeling sex, for example, as a node in a causal model aimed at elucidating fairness questions proposes two substantive claims: 1) There exists a feature, sex-on-its-own, that is an inherent trait of an individual that then (causally) brings about social phenomena external to it in the world; and 2) the relations between sex and its downstream effects can be modified in whichever ways and the former node would still retain the meaning that sex has in our world. Together, these claims suggest sex to be a category that could be different in its (causal) relations with other features of our social world via hypothetical interventions yet still mean what it means in our world. This fundamental stability of categories and causes (unless explicitly intervened on) is essential in the methodology of causal inference, because without it, causal operations can alter the meaning of a category, fundamentally change how it is situated within a causal diagram, and undermine the validity of any inferences drawn on the diagram as corresponding to any real phenomena in the world. We argue that these methods' ontological assumptions about social groups such as sex are conceptual errors. Many of the ""effects"" that sex purportedly ""causes"" are in fact constitutive features of sex as a social status. They constitute what it means to be sexed. In other words, together, they give the social meaning of sex features. These social meanings are precisely, we argue, what makes sex discrimination a distinctively morally problematic type of act that differs from mere irrationality or meanness on the basis of a physical feature. Correcting this conceptual error has a number of important implications for how analytical models can be used to detect discrimination. If what makes something discrimination on the basis of a particular social grouping is that the practice acts on what it means to be in that group in a way that we deem wrongful, then what we need from analytical diagrams is a model of what constitutes the social grouping. Such a model would allow us to explain the special moral (and legal) reasons we have to be concerned with the treatment of this category by reference to the empirical social relations and meanings that establish the category as what it is. Only then can we have the normative debate about what is fair or nondiscriminatory vis-√†-vis that group. We suggest that formal diagrams of constitutive relations would present an entirely different path toward reasoning about discrimination (and relatedly, counterfactuals) because they proffer a model of how the meaning of a social group emerges from its constitutive features. Whereas the value of causal diagrams is to guide the construction and testing of sophisticated modular counterfactuals, the value of constitutive diagrams would be to identify a different kind of counterfactual as central to our inquiry into discrimination: one that asks how the social meaning of a group would be changed if its non-modular features were altered.","['machine learning', 'algorithmic fairness', 'causal inference', 'discrimination', 'law', 'social philosophy']","['Theory of computation _ Design and analysis of algorithms', 'Applied computing _ Law, social and behavioral sciences']","['Lily Hu', 'Issa Kohler-Hausmann']","['Harvard University', 'Yale University']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372864,On the Apparent Conflict Between Individual and Group Fairness,Fairness & Bias,"A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.","['fairness', 'individual fairness', 'justice', 'machine learning', 'discrimina-tion', 'statistical parity']","['Social and professional topics _ Computing / technology policy', 'Codes of ethics', 'Computing methodologies _ Machine learning', 'Applied computing _ Law', 'Sociology']",['Reuben Binns'],['University of Oxford'],[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3372838,Measuring Justice in Machine Learning,Fairness & Bias,"How can we build more just machine learning systems? To answer this question, we need to know both what justice is and how to tell whether one system is more or less just than another. That is, we need both a definition and a measure of justice. Theories of distributive justice hold that justice can be measured (in part) in terms of the fair distribution of benefits and burdens across people in society. Recently, the field known as fair machine learning has turned to John Rawls's theory of distributive justice for inspiration and operationalization. However, philosophers known as capability theorists have long argued that Rawls's theory uses the wrong measure of justice, thereby encoding biases against people with disabilities. If these theorists are right, is it possible to operationalize Rawls's theory in machine learning systems without also encoding its biases? In this paper, I draw on examples from fair machine learning to suggest that the answer to this question is no: the capability theorists' arguments against Rawls's theory carry over into machine learning systems. But capability theorists don't only argue that Rawls's theory uses the wrong measure, they also offer an alternative measure. Which measure of justice is right? And has fair machine learning been using the wrong one?",[],[],['Alan Lundgard'],"['Massachusetts Institute of Technology Cambridge, Massachusetts']",[None]
https://dl.acm.org/doi/abs/10.1145/3351095.3372837,Reducing Sentiment Polarity for Demographic Attributes in Word Embeddings using Adversarial Learning,Security,"The use of word embedding models in sentiment analysis has gained a lot of traction in the Natural Language Processing (NLP) community. However, many inherently neutral word vectors describing demographic identity have unintended implicit correlations with negative or positive sentiment, resulting in unfair downstream machine learning algorithms. We leverage adversarial learning to decorrelate demographic identity term word vectors with positive or negative sentiment, and re-embed them into the word embeddings. We show that our method effectively minimizes unfair positive/negative sentiment polarity while retaining the semantic accuracy of the word embeddings. Furthermore, we show that our method effectively reduces unfairness in downstream sentiment regression and can be extended to reduce unfairness in toxicity classification tasks.","['embeddings', 'fairness', 'NLP']",['Computing methodologies _ Natural language processing'],"['Chris Sweeney', 'Maryam Najafian']","['MIT', 'MIT']","[None, None]"
https://dl.acm.org/doi/abs/10.1145/3351095.3372835,The Case for Voter-Centered Audits of Search Engines during Political Elections,Security,"Search engines, by ranking a few links ahead of million others based on opaque rules, open themselves up to criticism of bias. Previous research has focused on measuring political bias of search engine algorithms to detect possible search engine manipulation effects on voters or unbalanced ideological representation in search results. Insofar that these concerns are related to the principle of fairness, this notion of fairness can be seen as explicitly oriented toward election candidates or political processes and only implicitly oriented toward the public at large. Thus, we ask the following research question: how should an auditing framework that is explicitly centered on the principle of ensuring and maximizing fairness for the public (i.e., voters) operate? To answer this question, we qualitatively explore four datasets about elections and politics in the United States: 1) a survey of eligible U.S. voters about their information needs ahead of the 2018 U.S. elections, 2) a dataset of biased political phrases used in a large-scale Google audit ahead of the 2018 U.S. election, 3) Google's ""related searches"" phrases for two groups of political candidates in the 2018 U.S. election (one group is composed entirely of women), and 4) autocomplete suggestions and result pages for a set of searches on the day of a statewide election in the U.S. state of Virginia in 2019. We find that voters have much broader information needs than the search engine audit literature has accounted for in the past, and that relying on political science theories of voter modeling provides a good starting point for informing the design of voter-centered audits.","['algorithm audits', 'search engines', 'Google', 'voters', 'elections', 'bias']",['Information systems _ Web search engines'],"['Eni Mustafaraj', 'Emma Lurie', 'Claire Devine']","['Wellesley College', 'University of California', 'Wellesley College']","[None, None, None]"