year,doi_link,title,category,abstract,keywords,CCS concepts,author_names,affiliations,affiliated_countries
2022,https://doi.org/10.1145/3531146.3534639,A Review of Taxonomies of Explainable Artificial Intelligence (XAI) Methods,Transparency & Explainability,"The recent surge in publications related to explainable artificial intelligence (XAI) has led to an almost insurmountable wall if one wants to get started or stay up to date with XAI. For this reason, articles and reviews that present taxonomies of XAI methods seem to be a welcomed way to get an overview of the field. Building on this idea, there is currently a trend of producing such taxonomies, leading to several competing approaches to construct them. In this paper, we will review recent approaches to constructing taxonomies of XAI methods and discuss general challenges concerning them as well as their individual advantages and limitations. Our review is intended to help scholars be aware of challenges current taxonomies face. As we will argue, when charting the field of XAI, it may not be sufficient to rely on one of the approaches we found. To amend this problem, we will propose and discuss three possible solutions: a new taxonomy that incorporates the reviewed ones, a database of XAI methods, and a decision tree to help choose fitting methods.","['explainability', 'interpretability', 'explainable artificial intelligence', 'XAI', 'transparency', 'taxonomy', 'review']","['General and reference → Surveys and overviews', 'Computing methodologies → Artificial intelligence']",['Timo Speith'],"['Institute of Philosophy and Department of Computer Science, Saarland University']",['Germany']
2022,https://doi.org/10.1145/3531146.3533135,Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory,Transparency & Explainability,"Understanding how ML models work is a prerequisite for responsibly designing, deploying, and using ML-based systems. With interpretability approaches, ML can now offer explanations for its outputs to aid human understanding. Though these approaches rely on guidelines for how humans explain things to each other, they ultimately solve for improving the artifact—an explanation. In this paper, we propose an alternate framework for interpretability grounded in Weick’s sensemaking theory, which focuses on who the explanation is intended for. Recent work has advocated for the importance of understanding stakeholders’ needs—we build on this by providing concrete properties (e.g., identity, social context, environmental cues, etc.) that shape human understanding. We use an application of sensemaking in organizations as a template for discussing design guidelines for sensible AI, AI that factors in the nuances of human cognition when trying to explain itself.",,,"['Harmanpreet Kaur', 'Eytan Adar', 'Eric Gilbert', 'Cliff Lampe']","['University of Michigan', 'University of Michigan', 'University of Michigan', 'University of Michigan']","['USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533084,AI Opacity and Explainability in Tort Litigation,Transparency & Explainability,"A spate of recent accidents and a lawsuit involving Tesla's ‘self-driving’ cars highlights the growing need for meaningful accountability when harms are caused by AI systems. Tort (or civil liability) lawsuits are one important way for victims to redress such harms. The prospect of tort liability may also prompt AI developers to take better precautions against safety risks. Tort claims of all kinds will be hindered by AI opacity: the difficulty of determining how and why complex AI systems make decisions. We address this problem by formulating and evaluating several options for mitigating AI opacity that combine expert evidence, legal argumentation, civil procedure, and Explainable AI approaches. We emphasise the need for explanations of AI systems in tort litigation to be attuned to the elements of legal ‘causes of action’ – the specific facts that must be proven to succeed in a lawsuit. We take a recent Australian case involving explainable AI evidence as a starting point from which to map contemporary Explainable AI approaches to elements of tortious causes of action, focusing on misleading conduct, negligence, and product liability for safety defects. Our work synthesizes law, legal procedure, and computer science to provide greater clarity on the opportunities and challenges for Explainable AI in civil litigation, and may prove helpful to potential litigants, to courts, and to illuminate key targets for regulatory intervention.","['Explainable AI', 'Law', 'Evidence', 'Expert Evidence', 'AI Opacity', 'Civil Procedure', 'Negligence', 'Product Liability', 'Autonomous Vehicle', 'Accidents', 'Causation', 'Damages']","['Applied computing → Law', 'Computing methodologies → Machine learning', 'Computing methodologies → Artificial intelligence', 'Social and professional topics → Computing / technology policy']","['Henry Fraser', 'Rhyle Simcock', 'Aaron J. Snoswell']","['Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre', 'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre', 'Queensland University of Technology, Centre for Automated Decision-Making and Society, Australia and Queensland University of Technology, Digital Media Research Centre']","['Australia', 'Australia', 'Australia']"
2022,https://doi.org/10.1145/3531146.3533202,How Explainability Contributes to Trust in AI,Transparency & Explainability,"We provide a philosophical explanation of the relation between artificial intelligence (AI) explainability and trust in AI, providing a case for expressions, such as “explainability fosters trust in AI,” that commonly appear in the literature. This explanation relates the justification of the trustworthiness of an AI with the need to monitor it during its use. We discuss the latter by referencing an account of trust, called “trust as anti-monitoring,” that different authors contributed developing. We focus our analysis on the case of medical AI systems, noting that our proposal is compatible with internalist and externalist justifications of trustworthiness of medical AI and recent accounts of warranted contractual trust. We propose that “explainability fosters trust in AI” if and only if it fosters justified and warranted paradigmatic trust in AI, i.e., trust in the presence of the justified belief that the AI is trustworthy, which, in turn, causally contributes to rely on the AI in the absence of monitoring. We argue that our proposed approach can intercept the complexity of the interactions between physicians and medical AI systems in clinical practice, as it can distinguish between cases where humans hold different beliefs on the trustworthiness of the medical AI and exercise varying degrees of monitoring on them. Finally, we apply our account to user’s trust in AI, where, we argue, explainability does not contribute to trust. By contrast, when considering public trust in AI as used by a human, we argue, it is possible for explainability to contribute to trust. Our account can explain the apparent paradox that in order to trust AI, we must trust AI users not to trust AI completely. Summing up, we can explain how explainability contributes to justified trust in AI, without leaving a reliabilist framework, but only by redefining the trusted entity as an AI-user dyad.",,,"['Andrea Ferrario', 'Michele Loi']","['ETH Zurich', 'Politecnico di Milano']","['Switzerland', 'Italy']"
2022,https://doi.org/10.1145/3531146.3533090,It’s Just Not That Simple: An Empirical Study of the Accuracy-Explainability Trade-off in Machine Learning for Public Policy,Transparency & Explainability,"To achieve high accuracy in machine learning (ML) systems, practitioners often use complex “black-box” models that are not easily understood by humans. The opacity of such models has resulted in public concerns about their use in high-stakes contexts and given rise to two conflicting arguments about the nature — and even the existence — of the accuracy-explainability trade-off. One side postulates that model accuracy and explainability are inversely related, leading practitioners to use black-box models when high accuracy is important. The other side of this argument holds that the accuracy-explainability trade-off is rarely observed in practice and consequently, that simpler interpretable models should always be preferred. Both sides of the argument operate under the assumption that some types of models, such as low-depth decision trees and linear regression are more explainable, while others such as neural networks and random forests, are inherently opaque.  Our main contribution is an empirical quantification of the trade-off between model accuracy and explainability in two real-world policy contexts. We quantify explainability in terms of how well a model is understood by a human-in-the-loop (HITL) using a combination of objectively measurable criteria, such as a human’s ability to anticipate a model’s output or identify the most important feature of a model, and subjective measures, such as a human’s perceived understanding of the model. Our key finding is that explainability is not directly related to whether a model is a black-box or interpretable and is more nuanced than previously thought. We find that black-box models may be as explainable to a HITL as interpretable models and identify two possible reasons: (1) that there are weaknesses in the intrinsic explainability of interpretable models and (2) that more information about a model may confuse users, leading them to perform worse on objectively measurable explainability tasks. In summary, contrary to both positions in the literature, we neither observed a direct trade-off between accuracy and explainability nor found interpretable models to be superior in terms of explainability. It’s just not that simple!","['machine learning', 'explainability', 'public policy', 'responsible AI']","['Human-centered computing → Human computer interaction (HCI)', 'Computing methodologies → Machine learning']","['Andrew Bell', 'Ian Solano-Kamaiko', 'Oded Nov', 'Julia Stoyanovich']","['New York University', 'New York University', 'New York University', 'New York University']","['USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533236,Marrying Fairness and Explainability in Supervised Learning,Transparency & Explainability,"Machine learning algorithms that aid human decision-making may inadvertently discriminate against certain protected groups. Therefore, we formalize direct discrimination as a direct causal effect of the protected attributes on the decisions, while induced discrimination as a change in the causal influence of non-protected features associated with the protected attributes. The measurements of marginal direct effect (MDE) and SHapley Additive exPlanations (SHAP) reveal that state-of-the-art fair learning methods can induce discrimination via association or reverse discrimination in synthetic and real-world datasets. To inhibit discrimination in algorithmic systems, we propose to nullify the influence of the protected attribute on the output of the system, while preserving the influence of remaining features. We introduce and study post-processing methods achieving such objectives, finding that they yield relatively high model accuracy, prevent direct discrimination, and diminishes various disparity measures, e.g., demographic disparity.","['machine learning', 'explainability', 'algorithmic fairness', 'discrimination', 'supervised learning']","['Computing methodologies → Machine learning algorithms', 'Applied computing~Law, social and behavioral sciences;', 'Computing methodologies → Supervised learning']","['Przemyslaw A. Grabowicz', 'Nicholas Perello', 'Aarshee Mishra']","['College of Information and Computer Sciences, University of Massachusetts Amherst', 'University of Massachusetts Amherst', 'University of Massachusetts Amherst']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533179,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,Transparency & Explainability,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","['explainability', 'machine learning', 'fairness']","['Computing methodologies → Machine learning', 'Human-centred computing → explanations']","['Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute']","['USA', 'USA', 'Canada', 'USA', 'Canada', 'Canada']"
2022,https://doi.org/10.1145/3531146.3533133,Disclosure by Design: Designing information disclosures to support meaningful transparency and accountability,Transparency & Explainability,"There is a strong push for organisations to become more transparent and accountable for their undertakings. Towards this, various transparency regimes oblige organisations to disclose certain information to relevant stakeholders (individuals, regulators, etc). This information intends to empower and support the monitoring, oversight, scrutiny and challenge of organisational practices. Importantly, however, these disclosures are of limited benefit if they are not meaningful for their recipients. Yet, in practice, the disclosures of tech/data-driven organisations are often highly technical, fragmented, and therefore of limited utility to all but experts. This undermines a disclosure’s effectiveness, works to disempower, and ultimately hinders broader transparency aims.  This paper argues for a paradigm shift towards reconceptualising disclosures as ‘interfaces’ – designed for the needs, expectations and requirements of the recipients they serve to inform. In making this case, and to provide a practical way forward, we demonstrate Document Engineering as one potential methodology for specifying, designing, and deploying more effective information disclosures. Focusing on data protection disclosures, we illustrate and explore how designing disclosures as interfaces can better support greater oversight of organisational data and practices, and thus better align with broader transparency and accountability aims.","['transparency', 'accountability', 'GDPR', 'document engineering', 'interfaces', 'data rights', 'usability']","['Security and privacy → Human and societal aspects of security and privacy', 'Human-centered computing', 'Human-centered computing → Interaction design']","['Chris Norval', 'Kristin Cornelius', 'Jennifer Cobbe', 'Jatinder Singh']","['Compliant & Accountable Systems Group, University of Cambridge', 'Informatics Department, UCLA, Thousand Oaks, California, United States', 'Compliant & Accountable Systems Group, University of Cambridge', 'Compliant & Accountable Systems Group, University of Cambridge']","['United Kingdom', 'USA', 'United Kingdom', 'United Kingdom']"
2022,https://doi.org/10.1145/3531146.3533116,Goodbye Tracking? Impact of iOS App Tracking Transparency and Privacy Labels,Transparency & Explainability,"Tracking is a highly privacy-invasive data collection practice that has been ubiquitous in mobile apps for many years due to its role in supporting advertising-based revenue models. In response, Apple introduced two significant changes with iOS 14: App Tracking Transparency (ATT), a mandatory opt-in system for enabling tracking on iOS, and Privacy Nutrition Labels, which disclose what kinds of data each app processes. So far, the impact of these changes on individual privacy and control has not been well understood. This paper addresses this gap by analysing two versions of 1,759 iOS apps from the UK App Store: one version from before iOS 14 and one that has been updated to comply with the new rules.  We find that Apple’s new policies, as promised, prevent the collection of the Identifier for Advertisers (IDFA), an identifier for cross-app tracking. Smaller data brokers that engage in invasive data practices will now face higher challenges in tracking users – a positive development for privacy. However, the number of tracking libraries has – on average – roughly stayed the same in the studied apps. Many apps still collect device information that can be used to track users at a group level (cohort tracking) or identify individuals probabilistically (fingerprinting). We find real-world evidence of apps computing and agreeing on a fingerprinting-derived identifier through the use of server-side code, thereby violating Apple’s policies. We find that Apple itself engages in some forms of tracking and exempts invasive data practices like first-party tracking and credit scoring from its new tracking rules. We also find that the new Privacy Nutrition Labels are sometimes inaccurate and misleading, especially in less popular apps.  Overall, our observations suggest that, while Apple’s changes make tracking individual users more difficult, they motivate a countermovement, and reinforce existing market power of gatekeeper companies with access to large troves of first-party data. Making the privacy properties of apps transparent through large-scale analysis remains a difficult target for independent researchers, and a key obstacle to meaningful, accountable and verifiable privacy protections.",,,"['Konrad Kollnig', 'Anastasia Shuba', 'Max Van Kleek', 'Reuben Binns', 'Nigel Shadbolt']","['University of Oxford', 'Independent Researcher', 'University of Oxford', 'University of Oxford', 'University of Oxford']","['United Kingdom', 'USA', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
2022,https://doi.org/10.1145/3531146.3533239,Healthsheet: Development of a Transparency Artifact for Health Datasets,Transparency & Explainability,"Machine learning (ML) approaches have demonstrated promising results in a wide range of healthcare applications. Data plays a crucial role in developing ML-based healthcare systems that directly affect people’s lives. Many of the ethical issues surrounding the use of ML in healthcare stem from structural inequalities underlying the way we collect, use, and handle data. Developing guidelines to improve documentation practices regarding the creation, use, and maintenance of ML healthcare datasets is therefore of critical importance. In this work, we introduce Healthsheet, a contextualized adaptation of the original datasheet questionnaire  [22] for health-specific applications. Through a series of semi-structured interviews, we adapt the datasheets for healthcare data documentation. As part of the Healthsheet development process and to understand the obstacles researchers face in creating datasheets, we worked with three publicly-available healthcare datasets as our case studies, each with different types of structured data: Electronic health Records (EHR), clinical trial study data, and smartphone-based performance outcome measures. Our findings from the interviewee study and case studies show 1) that datasheets should be contextualized for healthcare, 2) that despite incentives to adopt accountability practices such as datasheets, there is a lack of consistency in the broader use of these practices 3) how the ML for health community views datasheets and particularly Healthsheets as diagnostic tool to surface the limitations and strength of datasets and 4) the relative importance of different fields in the datasheet to healthcare concerns.",[],['Human-centered computing → Heuristic evaluations'],"['Negar Rostamzadeh', 'Diana Mincu', 'Subhrajit Roy', 'Andrew Smart', 'Lauren Wilcox', 'Mahima Pushkarna', 'Jessica Schrouff', 'Razvan Amironesei', 'Nyalleng Moorosi', 'Katherine Heller']","['Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google']","['Canada', 'United Kingdom', 'United Kingdom', 'USA', 'USA', 'Canada', 'United Kingdom', 'USA', 'Ghana', 'USA']"
2022,https://doi.org/10.1145/3531146.3533204,Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,Transparency & Explainability,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",[],[],"['Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho']","['Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533174,Auditing for Gerrymandering by Identifying Disenfranchised Individuals,Transparency & Explainability,"Gerrymandering is the practice of drawing congressional districts to advantage or disadvantage particular electoral outcomes or population groups. We study the problem of computationally auditing a districting for evidence of gerrymandering. Our approach is novel in its emphasis on identifying individual voters disenfranchised by packing and cracking in local fine-grained geographic regions. We define a local score based on comparison with a representative sample of alternative districtings and use simulated annealing to algorithmically generate a witness districting to show that the score can be substantially reduced by simple local alterations. Unlike commonly studied metrics for gerrymandering such as proportionality and compactness, our framework is inspired by the legal context for voting rights in the United States. We demonstrate the use of our framework to analyze the congressional districting of the state of North Carolina in 2016. We identify a substantial number of geographically localized disenfranchised individuals, mostly Democrats in the central and north-eastern parts of the state. Our simulated annealing algorithm is able to generate a witness districting with a roughly 50% reduction in the number of disenfranchised individuals, suggesting that the 2016 districting was not predetermined by North Carolina’s spatial structure.",,,"['Jerry Lin', 'Carolyn Chen', 'Marc Chmielewski', 'Samia Zaman', 'Brandon Fain']","['Duke University', 'Duke University', 'Duke University', 'Duke University', 'Computer Science, Duke University']","['USA', 'USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533213,Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,Transparency & Explainability,"Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.",,,"['Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini']","['Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533115,Affirmative Algorithms: Relational Equality as Algorithmic Fairness,Fairness & Bias,"Many statistical fairness notions have been proposed for algorithmic decision-making systems, and especially public safety pretrial risk assessment (PSPRA) algorithms such as COMPAS. Most fairness notions equalize something between groups, whether it is false positive rates or accuracy. In fact, I demonstrate that most prominent notions have their basis in equalizing some form of accuracy. However, statistical fairness metrics often do not capture the substantive point of equality. I argue that equal accuracy is not only difficult to measure but also unsatisfactory for ensuring equal justice. In response, I introduce philosopher Elizabeth Anderson’s theory of relational equality as a fruitful alternative framework: to relate as equals, people need access to certain basic capabilities. I show that relational equality requires Affirmative PSPRA algorithms that lower risk scores for Black defendants. This is because fairness based on relational equality means considering the impact of PSPRA algorithms’ decisions on access to basic capabilities. This impact is racially asymmetric in an unjust society. I make three main contributions: (1) I illustrate the shortcomings of statistical fairness notions in their reliance on equalizing some form of accuracy; (2) I present the first comprehensive ethical defense of Affirmative PSPRA algorithms, based on fairness in terms of relational equality instead; and (3) I show that equalizing accuracy is neither sufficient nor necessary for fairness based on relational equality. Overall, this work serves narrowly as a reason to re-evaluate algorithmic fairness for PSPRA algorithms, and serves broadly as an example of how discussions of algorithmic fairness can benefit from egalitarian philosophical frameworks.",,,['Marilyn Zhang'],['Stanford University'],['USA']
2022,https://doi.org/10.1145/3531146.3533204,Algorithmic Fairness and Vertical Equity: Income Fairness with IRS Tax Audit Models,Fairness & Bias,"This study examines issues of algorithmic fairness in the context of systems that inform tax audit selection by the United States Internal Revenue Service (IRS). While the field of algorithmic fairness has developed primarily around notions of treating like individuals alike, we instead explore the concept of vertical equity—appropriately accounting for relevant differences across individuals—which is a central component of fairness in many public policy settings. Applied to the design of the U.S. individual income tax system, vertical equity relates to the fair allocation of tax and enforcement burdens across taxpayers of different income levels. Through a unique collaboration with the Treasury Department and IRS, we use access to detailed, anonymized individual taxpayer microdata, risk-selected audits, and random audits from 2010-14 to study vertical equity in tax administration. In particular, we assess how the adoption of modern machine learning methods for selecting taxpayer audits may affect vertical equity. Our paper makes four contributions. First, we show how the adoption of more flexible machine learning (classification) methods—as opposed to simpler models—shapes vertical equity by shifting audit burdens from high to middle-income taxpayers. Second, given concerns about high audit rates of low-income taxpayers, we investigate how existing algorithmic fairness techniques would change the audit distribution. We find that such methods can mitigate some disparities across income buckets, but that these come at a steep cost to performance. Third, we show that the choice of whether to treat risk of underreporting as a classification or regression problem is highly consequential. Moving from a classification approach to a regression approach to predict the expected magnitude of underreporting shifts the audit burden substantially toward high income individuals, while increasing revenue. Last, we investigate the role of differential audit cost in shaping the distribution of audits. Audits of lower income taxpayers, for instance, are typically conducted by mail and hence pose much lower cost to the IRS. We show that a narrow focus on return-on-investment can undermine vertical equity. Our results have implications for ongoing policy debates and the design of algorithmic tools across the public sector.",[],[],"['Emily Black', 'Hadi Elzayn', 'Alexandra Chouldechova', 'Jacob Goldin', 'Daniel Ho']","['Computer Science Dept., Carngie Mellon University', 'Stanford University', 'Carnegie Mellon University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533226,Demographic-Reliant Algorithmic Fairness: Characterizing the Risks of Demographic Data Collection in the Pursuit of Fairness,Fairness & Bias,"Most proposed algorithmic fairness techniques require access to demographic data in order to make performance comparisons and standardizations across groups, however this data is largely unavailable in practice, hindering the widespread adoption of algorithmic fairness. Through this paper, we consider calls to collect more data on demographics to enable algorithmic fairness and challenge the notion that discrimination can be overcome with smart enough technical methods and sufficient data. We show how these techniques largely ignore broader questions of data governance and systemic oppression when categorizing individuals for the purpose of fairer algorithmic processing. In this work, we explore under what conditions demographic data should be collected and used to enable algorithmic fairness methods by characterizing a range of social risks to individuals and communities. For the risks to individuals we consider the unique privacy risks of sensitive attributes, the possible harms of miscategorization and misrepresentation, and the use of sensitive data beyond data subjects’ expectations. Looking more broadly, the risks to entire groups and communities include the expansion of surveillance infrastructure in the name of fairness, misrepresenting and mischaracterizing what it means to be part of a demographic group, and ceding the ability to define what constitutes biased or unfair treatment. We argue that, by confronting these questions before and during the collection of demographic data, algorithmic fairness methods are more likely to actually mitigate harmful treatment disparities without reinforcing systems of oppression. Towards this end, we assess privacy-focused methods of data collection and use and participatory data governance structures as proposals for more responsibly collecting demographic data.",,,"['McKane Andrus', 'Sarah Villeneuve']","['Partnership on AI', 'Partnership on AI']","['USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533166,"Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare",Fairness & Bias,"A growing body of work uses the paradigm of algorithmic fairness to frame the development of techniques to anticipate and proactively mitigate the introduction or exacerbation of health inequities that may follow from the use of model-guided decision-making. We evaluate the interplay between measures of model performance, fairness, and the expected utility of decision-making to offer practical recommendations for the operationalization of algorithmic fairness principles for the development and evaluation of predictive models in healthcare. We conduct an empirical case-study via development of models to estimate the ten-year risk of atherosclerotic cardiovascular disease to inform statin initiation in accordance with clinical practice guidelines. We demonstrate that approaches that incorporate fairness considerations into the model training objective typically do not improve model performance or confer greater net benefit for any of the studied patient populations compared to the use of standard learning paradigms followed by threshold selection concordant with patient preferences, evidence of intervention effectiveness, and model calibration. These results hold when the measured outcomes are not subject to differential measurement error across patient populations and threshold selection is unconstrained, regardless of whether differences in model performance metrics, such as in true and false positive error rates, are present. In closing, we argue for focusing model development efforts on developing calibrated models that predict outcomes well for all patient populations while emphasizing that such efforts are complementary to transparent reporting, participatory design, and reasoning about the impact of model-informed interventions in context.",,,"['Stephen Pfohl', 'Yizhe Xu', 'Agata Foryciarz', 'Nikolaos Ignatiadis', 'Julian Genkins', 'Nigam Shah']","['Stanford University, USA and Google', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University', 'Stanford University']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533211,ABCinML: Anticipatory Bias Correction in Machine Learning Applications,Fairness & Bias,"The idealization of a static machine-learned model, trained once and deployed forever, is not practical. As input distributions change over time, the model will not only lose accuracy, any constraints to reduce bias against a protected class may fail to work as intended. Thus, researchers have begun to explore ways to maintain algorithmic fairness over time. One line of work focuses on dynamic learning: retraining after each batch, and the other on robust learning which tries to make algorithms robust against all possible future changes. Dynamic learning seeks to reduce biases soon after they have occurred and robust learning often yields (overly) conservative models. We propose an anticipatory dynamic learning approach for correcting the algorithm to mitigate bias before it occurs. Specifically, we make use of anticipations regarding the relative distributions of population subgroups (e.g., relative ratios of male and female applicants) in the next cycle to identify the right parameters for an importance weighing fairness approach. Results from experiments over multiple real-world datasets suggest that this approach has promise for anticipatory bias correction.",,,"['Abdulaziz A. Almuzaini', 'Chidansh A. Bhatt', 'David M. Pennock', 'Vivek K. Singh']","['Rutgers University', 'IBM', 'Rutgers University', 'Rutgers University']","['USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533172,An Algorithmic Framework for Bias Bounties,Fairness & Bias,"We propose and analyze an algorithmic framework for “bias bounties” — events in which external participants are invited to propose improvements to a trained model, akin to bug bounty events in software and security. Our framework allows participants to submit arbitrary subgroup improvements, which are then algorithmically incorporated into an updated model. Our algorithm has the property that there is no tension between overall and subgroup accuracies, nor between different subgroup accuracies, and it enjoys provable convergence to either the Bayes optimal model or a state in which no further improvements can be found by the participants. We provide formal analyses of our framework, experimental evaluation, and findings from a preliminary bias bounty event.1",,,"['Ira Globus-Harris', 'Michael Kearns', 'Aaron Roth']","['University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon', 'University of Pennsylvania/Amazon']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533089,Bias in Automated Speaker Recognition,Fairness & Bias,"Automated speaker recognition uses data processing to identify speakers by their voice. Today, automated speaker recognition is deployed on billions of smart devices and in services such as call centres. Despite their wide-scale deployment and known sources of bias in related domains like face recognition and natural language processing, bias in automated speaker recognition has not been studied systematically. We present an in-depth empirical and analytical study of bias in the machine learning development workflow of speaker verification, a voice biometric and core task in automated speaker recognition. Drawing on an established framework for understanding sources of harm in machine learning, we show that bias exists at every development stage in the well-known VoxCeleb Speaker Recognition Challenge, including data generation, model building, and implementation. Most affected are female speakers and non-US nationalities, who experience significant performance degradation. Leveraging the insights from our findings, we make practical recommendations for mitigating bias in automated speaker recognition, and outline future research directions.","['speaker recognition', 'speaker verification', 'bias', 'fairness', 'audit', 'evaluation']","['General and reference → Evaluation', 'Security and privacy → Biometrics', 'Computing methodologies → Speech recognition', 'Computing methodologies~Machine learning;']","['Wiebke Toussaint Hutiri', 'Aaron Yi Ding']","['Technology, Policy & Management / Engineering Systems & Services / Cyber Physical Intelligence Lab, Delft University of Technology', 'Technology, Policy & Management/Engineering Systems & Services / Cyber Physical Intelligence Lab, Delft University of Technology']","['Netherlands', 'Netherlands']"
2022,https://doi.org/10.1145/3531146.3534644,Data augmentation for fairness-aware machine learning: Preventing algorithmic bias in law enforcement systems,Fairness & Bias,"Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.","['Computing methodologies', 'Machine learning', 'Social and professional topics∼Computing / technology policy', 'Surveillance∼Governmental surveillance', 'computer vision', 'fairness', 'algorithmic bias', 'AI ethics', 'violence detection', 'law enforcement technology']",[],"['Ioannis Pastaltzidis', 'Nikolaos Dimitriou', 'Katherine Quezada-Tavarez', 'Stergios Aidinlis', 'Thomas Marquenie', 'Agata Gurzawska', 'Dimitrios Tzovaras']","['Information Technologies Institute, Centre for Research and Technology Hellas', 'Information Technologies Institute, Centre for Research and Technology Hellas', 'Centre for IT and IP Law, KU Leuven', 'Trilateral Research', 'Centre for IT and IP Law, KU Leuven', 'Trilateral Research', 'Information Technologies Institute, Centre for Research and Technology Hellas']","['Greece', 'Greece', 'Belgium', 'United Kingdom', 'Belgium', 'United Kingdom', 'Greece']"
2022,https://doi.org/10.1145/3531146.3533105,De-biasing “bias” measurement,Fairness & Bias,"When a model’s performance differs across socially or culturally relevant groups–like race, gender, or the intersections of many such groups–it is often called ”biased.” While much of the work in algorithmic fairness over the last several years has focused on developing various definitions of model fairness (the absence of group-wise model performance disparities) and eliminating such “bias,” much less work has gone into rigorously measuring it. In practice, it important to have high quality, human digestible measures of model performance disparities and associated uncertainty quantification about them that can serve as inputs into multi-faceted decision-making processes. In this paper, we show both mathematically and through simulation that many of the metrics used to measure group-wise model performance disparities are themselves statistically biased estimators of the underlying quantities they purport to represent. We argue that this can cause misleading conclusions about the relative group-wise model performance disparities along different dimensions, especially in cases where some sensitive variables consist of categories with few members. We propose the “double-corrected” variance estimator, which provides unbiased estimates and uncertainty quantification of the variance of model performance across groups. It is conceptually simple and easily implementable without statistical software package or numerical optimization. We demonstrate the utility of this approach through simulation and show on a real dataset that while statistically biased estimators of model group-wise model performance disparities indicate statistically significant between-group model performance disparities, when accounting for statistical bias in the estimator, the estimated group-wise disparities in model performance are no longer statistically significant.",,,"['Kristian Lum', 'Yunfeng Zhang', 'Amanda Bower']","['Twitter', 'Twitter', 'Twitter']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533129,Don’t let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution to the Hiring Paradox,Fairness & Bias,"Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department’s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too much action.  In this work, we give a powerful way to solve this hiring paradox that tracks both legal and algorithmic challenges. We unpack the nuances of Ricci v. DeStefano and extend the legal literature arguing that certain algorithmic approaches to employment are allowed by introducing the legal practice of banding to evaluate candidates. We thus show that a bias-aware technique can be used to diagnose and mitigate “built-in” headwinds in the employment pipeline. We use the machinery of partially ordered sets to handle the presence of uncertainty in evaluations data. This approach allows us to move away from treating “people as numbers” to treating people as individuals—a property that is sought after by Title VII in the context of employment.","['anti-discrimination laws', 'hiring', 'resume screening', 'bias', 'uncertainty']","['Applied computing → Law', 'Mathematics of computing → Discrete mathematics']","['Jad Salem', 'Deven Desai', 'Swati Gupta']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'Georgia Institute of Technology']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533144,Exploring the Role of Grammar and Word Choice in Bias Toward African American English (AAE) in Hate Speech Classification,Fairness & Bias,"Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.","['Natural Language Processing', 'Linguistics', 'Fairness', 'African American English', 'Hate Speech', 'Social Media']","['Computing methodologies → Natural language processing', 'Computing methodologies → Information extraction', 'Computing methodologies → Machine learning', 'Social and professional topics → Race and ethnicity']","['Camille Harris', 'Matan Halevy', 'Ayanna Howard', 'Amy Bruckman', 'Diyi Yang']","['Georgia Institute of Technology', 'Georgia Institute of Technology', 'The Ohio State University', 'Georgia Institute of Technology', 'Georgia Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533159,"Female, white, 27? Bias Evaluation on Data and Algorithms for Affect Recognition in Faces",Fairness & Bias,"Nowadays, Artificial Intelligence (AI) algorithms show a strong performance for many use cases, making them desirable for real-world scenarios where the algorithms provide high-impact decisions. However, one major drawback of AI algorithms is their susceptibility to bias and resulting unfairness. This has a huge influence for their application, as they have a higher failure rate for certain subgroups. In this paper, we focus on the field of affective computing and particularly on the detection of bias for facial expressions. Depending on the deployment scenario, bias in facial expression models can have a disadvantageous impact and it is therefore essential to evaluate the bias and limitations of the model. In order to analyze the metadata distribution in affective computing datasets, we annotate several benchmark training datasets, containing both Action Units and categorical emotions, with age, gender, ethnicity, glasses, and beards. We show that there is a significantly skewed distribution, particularly for ethnicity and age. Based on this metadata annotation, we evaluate two trained state-of-the-art affective computing algorithms. Our evaluation shows that the strongest bias is in age, with the best performance for persons under 34 and a sharp decrease for older persons. Furthermore, we see an ethnicity bias with varying direction depending on the algorithm, a slight gender bias and worse performance for facial parts occluded by glasses.","['affective computing', 'action units', 'categorical emotions', 'metadata post-annotation', 'bias', 'fairness', 'data evaluation', 'algorithm evaluation']","['Computing methodologies → Computer vision', 'Computing methodologies → Machine learning', 'Social and professional topics → Age', 'Social and professional topics → Race and ethnicity', 'Social and professional topics~Gender;']","['Jaspar Pahl', 'Ines Rieger', 'Anna Möller', 'Thomas Wittenberg', 'Ute Schmid']","['Multimodal Human Sensing, Fraunhofer-Institute for Integrated Circuits IIS, Germany and Cognitive Systems Group, University of Bamberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Cognitive Systems Group, University of Bamberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Chair for Visual Computing, Friedrich-Alexander-Universität Erlangen-Nürnberg', 'Fraunhofer-Institute for Integrated Circuits IIS, Germany and Chair for Visual Computing, Friedrich-Alexander-Universität Erlangen-Nürnberg', 'Cognitive Systems Group, University of Bamberg, Germany and Project Group Comprehensible AI, Fraunhofer-Institute for Integrated Circuits IIS']","['Germany', 'Germany', 'Germany', 'Germany', 'Germany']"
2022,https://doi.org/10.1145/3531146.3533184,Gender and Racial Bias in Visual Question Answering Datasets,Fairness & Bias,"Vision-and-language tasks have increasingly drawn more attention as a means to evaluate human-like reasoning in machine learning models. A popular task in the field is visual question answering (VQA), which aims to answer questions about images. However, VQA models have been shown to exploit language bias by learning the statistical correlations between questions and answers without looking into the image content: e.g., questions about the color of a banana are answered with yellow, even if the banana in the image is green. If societal bias (e.g., sexism, racism, ableism, etc.) is present in the training data, this problem may be causing VQA models to learn harmful stereotypes. For this reason, we investigate gender and racial bias in five VQA datasets. In our analysis, we find that the distribution of answers is highly different between questions about women and men, as well as the existence of detrimental gender-stereotypical samples. Likewise, we identify that specific race-related attributes are underrepresented, whereas potentially discriminatory samples appear in the analyzed datasets. Our findings suggest that there are dangers associated to using VQA datasets without considering and dealing with the potentially harmful stereotypes. We conclude the paper by proposing solutions to alleviate the problem before, during, and after the dataset collection process.",,,"['Yusuke Hirota', 'Yuta Nakashima', 'Noa Garcia']","['Osaka University', 'Osaka University', 'Osaka University']","['Japan', 'Japan', 'Japan']"
2022,https://doi.org/10.1145/3531146.3533117,Language variation and algorithmic bias: understanding algorithmic bias in British English automatic speech recognition,Fairness & Bias,"All language is characterised by variation which language users employ to construct complex social identities and express social meaning. Like other machine learning technologies, speech and language technologies (re)produce structural oppression when they perform worse for marginalised language communities. Using knowledge and theories from sociolinguistics, I explore why commercial automatic speech recognition systems and other language technologies perform significantly worse for already marginalised populations, such as second-language speakers and speakers of stigmatised varieties of English in the British Isles. Situating language technologies within the broader scholarship around algorithmic bias, consider the allocative and representational harms they can cause even (and perhaps especially) in systems which do not exhibit predictive bias, narrowly defined as differential performance between groups. This raises the question whether addressing or “fixing” this “bias” is actually always equivalent to mitigating the harms algorithmic systems can cause, in particular to marginalised communities.",,,['Nina Markl'],['University of Edinburgh'],['United Kingdom']
2022,https://doi.org/10.1145/3531146.3533124,Selection in the Presence of Implicit Bias: The Advantage of Intersectional Constraints,Fairness & Bias,"In selection processes such as hiring, promotion, and college admissions, implicit bias toward socially-salient attributes such as race, gender, or sexual orientation of candidates is known to produce persistent inequality and reduce aggregate utility for the decision maker. Interventions such as the Rooney Rule and its generalizations, which require the decision maker to select at least a specified number of individuals from each affected group, have been proposed to mitigate the adverse effects of implicit bias in selection. Recent works have established that such lower-bound constraints can be very effective in improving aggregate utility in the case when each individual belongs to at most one affected group. However, in several settings, individuals may belong to multiple affected groups and, consequently, face more extreme implicit bias due to this intersectionality. We consider independently drawn utilities and show that, in the intersectional case, the aforementioned non-intersectional constraints can only recover part of the total utility achievable in the absence of implicit bias. On the other hand, we show that if one includes appropriate lower-bound constraints on the intersections, almost all the utility achievable in the absence of implicit bias can be recovered. Thus, intersectional constraints can offer a significant advantage over a reductionist dimension-by-dimension non-intersectional approach to reducing inequality.",,,"['Anay Mehrotra', 'Bary S. R. Pradelski', 'Nisheeth K. Vishnoi']","['Yale University', 'National Centre for Scientific Research (CNRS)', 'Yale University']","['USA', 'France', 'USA']"
2022,https://doi.org/10.1145/3531146.3534629,The Effects of Crowd Worker Biases in Fact-Checking Tasks,Fairness & Bias,"Due to the increasing amount of information shared online every day, the need for sound and reliable ways of distinguishing between trustworthy and non-trustworthy information is as present as ever. One technique for performing fact-checking at scale is to employ human intelligence in the form of crowd workers. Although earlier work has suggested that crowd workers can reliably identify misinformation, cognitive biases of crowd workers may reduce the quality of truthfulness judgments in this context. We performed a systematic exploratory analysis of publicly available crowdsourced data to identify a set of potential systematic biases that may occur when crowd workers perform fact-checking tasks. Following this exploratory study, we collected a novel data set of crowdsourced truthfulness judgments to validate our hypotheses. Our findings suggest that workers generally overestimate the truthfulness of statements and that different individual characteristics (i.e., their belief in science) and cognitive biases (i.e., the affect heuristic and overconfidence) can affect their annotations. Interestingly, we find that, depending on the general judgment tendencies of workers, their biases may sometimes lead to more accurate judgments.","['Truthfulness', 'Crowdsourcing', 'Misinformation', 'Explainability', 'Bias']","['Information systems → Crowdsourcing', 'General and reference → Estimation']","['Tim Draws', 'David La Barbera', 'Michael Soprano', 'Kevin Roitero', 'Davide Ceolin', 'Alessandro Checco', 'Stefano Mizzaro']","['Delft University of Technology', 'University of Udine', 'University of Udine', 'University of Udine', 'CWI', 'University of Rome La Sapienza', 'University of Udine']","['Netherlands', 'Italy', 'Italy', 'Italy', 'Netherlands', 'Italy', 'Italy']"
2022,https://doi.org/10.1145/3531146.3533179,The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations,Fairness & Bias,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","['explainability', 'machine learning', 'fairness']","['Computing methodologies → Machine learning', 'Human-centred computing → explanations']","['Aparna Balagopalan', 'Haoran Zhang', 'Kimia Hamidieh', 'Thomas Hartvigsen', 'Frank Rudzicz', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute', 'Massachusetts Institute of Technology', 'University of Toronto/Vector Institute, Canada and Unity Health Toronto', 'Massachusetts Institute of Technology, USA and Vector Institute']","['USA', 'USA', 'Canada', 'USA', 'Canada', 'Canada']"
2022,https://doi.org/10.1145/3531146.3534627,Theories of “Gender” in NLP Bias Research,Fairness & Bias,"The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time.  We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.","['natural language processing', 'gender bias', 'gender studies']",[],"['Hannah Devinney', 'Jenny Björklund', 'Henrik Björklund']","['Umeå University', 'Uppsala University', 'Umeå University']","['Sweden', 'Sweden', 'Sweden']"
2022,https://doi.org/10.1145/3531146.3533102,An Outcome Test of Discrimination for Ranked Lists,Fairness & Bias,"This paper extends Becker [3]’s outcome test of discrimination to settings where a (human or algorithmic) decision-maker produces a ranked list of candidates. Ranked lists are particularly relevant in the context of online platforms that produce search results or feeds, and also arise when human decisionmakers express ordinal preferences over a list of candidates. We show that non-discrimination implies a system of moment inequalities, which intuitively impose that one cannot permute the position of a lower-ranked candidate from one group with a higher-ranked candidate from a second group and systematically improve the objective. Moreover, we show that that these moment inequalities are the only testable implications of non-discrimination when the auditor observes only outcomes and group membership by rank. We show how to statistically test the implied inequalities, and validate our approach in an application using data from LinkedIn.1",,,"['Jonathan Roth', 'Guillaume Saint-Jacques', 'YinYin Yu']","['Brown University', 'Apple', 'LinkedIn']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533160,Beyond Fairness: Reparative Algorithms to Address Historical Injustices of Housing Discrimination in the US,Fairness & Bias,"Fairness in Machine Learning (ML) has mostly focused on interrogating the fairness of a particular decision point with assumptions made that the people represented in the data have been fairly treated throughout history. However, fairness cannot be ultimately achieved if such assumptions are not valid. This is the case for mortgage lending discrimination in the US, which should be critically understood as the result of historically accumulated injustices that were enacted through public policies and private practices including redlining, racial covenants, exclusionary zoning, and predatory inclusion, among others. With the erroneous assumptions of historical fairness in ML, Black borrowers with low income and low wealth are considered as a given condition in a lending algorithm, thus rejecting loans to them would be considered a “fair” decision even though Black borrowers were historically excluded from homeownership and wealth creation. To emphasize such issues, we introduce case studies using contemporary mortgage lending data as well as historical census data in the US. First, we show that historical housing discrimination has differentiated each racial group’s baseline wealth which is a critical input for algorithmically determining mortgage loans. The second case study estimates the cost of housing reparations in the algorithmic lending context to redress historical harms because of such discriminatory housing policies. Through these case studies, we envision what reparative algorithms would look like in the context of housing discrimination in the US. This work connects to emerging scholarship on how algorithmic systems can contribute to redressing past harms through engaging with reparations policies and programs.","['fairness', 'mortgage lending', 'housing', 'racial wealth gap', 'reparations']","['Theory of computation → Design and analysis of algorithms', 'Applied computing → Sociology']","['Wonyoung So', 'Pranay Lohia', 'Rakesh Pimplikar', 'A.E. Hosoi', ""Catherine D'Ignazio""]","['Department of Urban Studies and Planning, Massachusetts Institute of Technology', 'Microsoft', 'IBM Research AI', 'Institute for Data, Systems, and Society, Massachusetts Institute of Technology', 'Department of Urban Studies and Planning, Massachusetts Institute of Technology']","['USA', 'India', 'India', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533169,"Tackling Algorithmic Disability Discrimination in the Hiring Process: An Ethical, Legal and Technical Analysis",Fairness & Bias,"Tackling algorithmic discrimination against persons with disabilities (PWDs) demands a distinctive approach that is fundamentally different to that applied to other protected characteristics, due to particular ethical, legal, and technical challenges. We address these challenges specifically in the context of artificial intelligence (AI) systems used in hiring processes (or automated hiring systems, AHSs), in which automated assessment procedures are subject to unique ethical and legal considerations and have an undeniable adverse impact on PWDs. In this paper, we discuss concerns and opportunities raised by AI-driven hiring in relation to disability discrimination. Ultimately, we aim to encourage further research into this topic. Hence, we establish some starting points and design a roadmap for ethicists, lawmakers, advocates as well as AI practitioners alike.",,,"['Maarten Buyl', 'Christina Cociancig', 'Cristina Frattone', 'Nele Roekens']","['Ghent University', 'University of Bremen', 'Roma Tre University', 'Unia']","['Belgium', 'Germany', 'Italy', 'Belgium']"
2022,https://doi.org/10.1145/3531146.3533242,What is Proxy Discrimination?,Fairness & Bias,"The near universal condemnation of proxy discrimination hides a disagreement over what it is. This work surveys various notions of proxy and proxy discrimination found in prior work and represents them in a common framework. These notions variously turn on statistical dependencies, causal effects, and intentions. It discusses the limitations and uses of each notation and of the concept as a whole.","['proxy', 'discrimination', 'conceptual analysis']",['Social and professional topics → Computing / technology policy'],['Michael Carl Tschantz'],['International Computer Science Institute'],['USA']
2022,https://doi.org/10.1145/3531146.3533221,Human-Algorithm Collaboration: Achieving Complementarity and Avoiding Unfairness,Fairness & Bias,"Much of machine learning research focuses on predictive accuracy: given a task, create a machine learning model (or algorithm) that maximizes accuracy. In many settings, however, the final prediction or decision of a system is under the control of a human, who uses an algorithm’s output along with their own personal expertise in order to produce a combined prediction. One ultimate goal of such collaborative systems is complementarity: that is, to produce lower loss (equivalently, greater payoff or utility) than either the human or algorithm alone. However, experimental results have shown that even in carefully-designed systems, complementary performance can be elusive. Our work provides three key contributions. First, we provide a theoretical framework for modeling simple human-algorithm systems and demonstrate that multiple prior analyses can be expressed within it. Next, we use this model to prove conditions where complementarity is impossible, and give constructive examples of where complementarity is achievable. Finally, we discuss the implications of our findings, especially with respect to the fairness of a classifier. In sum, these results deepen our understanding of key factors influencing the combined performance of human-algorithm systems, giving insight into how algorithmic tools can best be designed for collaborative environments.",[],[],"['Kate Donahue', 'Alexandra Chouldechova', 'Krishnaram Kenthapadi']","['Cornell University', 'Amazon', 'Fiddler AI']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3534645,Enforcing Group Fairness in Algorithmic Decision Making: Utility Maximization Under Sufficiency,Fairness & Bias,"Binary decision making classifiers are not fair by default. Fairness requirements are an additional element to the decision making rationale, which is typically driven by maximizing some utility function. In that sense, algorithmic fairness can be formulated as a constrained optimization problem. This paper contributes to the discussion on how to implement fairness, focusing on the fairness concepts of positive predictive value (PPV) parity, false omission rate (FOR) parity, and sufficiency (which combines the former two).  We show that group-specific threshold rules are optimal for PPV parity and FOR parity, similar to well-known results for other group fairness criteria. However, depending on the underlying population distributions and the utility function, we find that sometimes an upper-bound threshold rule for one group is optimal: utility maximization under PPV parity (or FOR parity) might thus lead to selecting the individuals with the smallest utility for one group, instead of selecting the most promising individuals. This result is counter-intuitive and in contrast to the analogous solutions for statistical parity and equality of opportunity.  We also provide a solution for the optimal decision rules satisfying the fairness constraint sufficiency. We show that more complex decision rules are required and that this leads to within-group unfairness for all but one of the groups. We illustrate our findings based on simulated and real data.","['algorithmic fairness', 'prediction-based decision making', 'constrained utility optimization', 'sufficiency', 'machine learning', 'group fairness metrics', 'fairness trade-offs']","['Computing methodologies → Machine learning', 'Applied computing → Decision analysis']","['Joachim Baumann', 'Anikó Hannák', 'Christoph Heitz']","['Universitiy of Zurich, Switzerland and Zurich University of Applied Sciences', 'Universitiy of Zurich', 'Zurich University of Applied Sciences']","['Switzerland', 'Switzerland', 'Switzerland']"
2022,https://doi.org/10.1145/3531146.3533081,Minimax Demographic Group Fairness in Federated Learning,Fairness & Bias,"Federated learning is an increasingly popular paradigm that enables a large number of entities to collaboratively learn better models. In this work, we study minimax group fairness in federated learning scenarios where different participating entities may only have access to a subset of the population groups during the training phase. We formally analyze how our proposed group fairness objective differs from existing federated learning fairness criteria that impose similar performance across participants instead of demographic groups. We provide an optimization algorithm – FedMinMax – for solving the proposed problem that provably enjoys the performance guarantees of centralized learning algorithms. We experimentally compare the proposed approach against other state-of-the-art methods in terms of group fairness in various federated learning setups, showing that our approach exhibits competitive or superior performance.","['Federated Learning', 'Algorithmic Fairness', 'Minimax Group Fairness']","['Computing methodologies → Machine learning', 'Computing methodologies → Cooperation and coordination']","['Afroditi Papadaki', 'Natalia Martinez', 'Martin Bertran', 'Guillermo Sapiro', 'Miguel Rodrigues']","['University College London', 'Duke University', 'Duke University', 'Duke University, USA and Apple Inc.', 'University College London']","['United Kingdom', 'USA', 'USA', 'USA', 'United Kingdom']"
2022,https://doi.org/10.1145/3531146.3533171,Trade-offs between Group Fairness Metrics in Societal Resource Allocation,Fairness & Bias,"We consider social resource allocations that deliver an array of scarce supports to a diverse population. Such allocations pervade social service delivery, such as provision of homeless services and assignment of refugees to cities, among others. At issue is whether allocations are fair across sociodemographic groups and intersectional identities. Our paper shows that necessary trade-offs exist for fairness in the context of scarcity; many reasonable definitions of equitable outcomes cannot hold simultaneously except under stringent conditions. For example, defining fairness in terms of improvement over a baseline inherently conflicts with defining fairness in terms of loss compared with the best possible outcome. Moreover, we demonstrate that the fairness trade-offs stem from heterogeneity across groups in intervention responses. Administrative records on homeless service delivery offer a real-world example. Building on prior work, we measure utilities for each household as the probability of reentry into homeless services if given three homeless services. Heterogeneity in utility distributions (conditional on received services) for several sociodemographic groups (e.g. single women with children versus without children) generates divergence across fairness metrics. We argue that such heterogeneity, and thus, fairness trade-offs, pervade many social policy contexts.","['Resource allocation', 'algorithmic fairness', 'fairness metrics']","['Computing methodologies → Artificial intelligence', 'General and reference → Metrics', 'General and reference → Evaluation']","['Tasfia Mashiat', 'Xavier Gitiaux', 'Huzefa Rangwala', 'Patrick Fowler', 'Sanmay Das']","['George Mason University', 'George Mason University', 'George Mason University', 'Washington University in St. Louis', 'George Mason University']","['USA', 'USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533079,Providing Item-side Individual Fairness for Deep Recommender Systems,Fairness & Bias,"Recent advent of deep learning techniques have reinforced the development of new recommender systems. Although these systems have been demonstrated as efficient and effective, the issue of item popularity bias in these recommender systems has raised serious concerns. While most of the existing works focus on group fairness at item side, individual fairness at item side is left largely unexplored. To address this issue, in this paper, first, we define a new notion of individual fairness from the perspective of items, namely (α, β)-fairness, to deal with item popularity bias in recommendations. In particular, (α, β)-fairness requires that similar items should receive similar coverage in the recommendations, where α and β control item similarity and coverage similarity respectively, and both item and coverage similarity metrics are defined as task specific for deep recommender systems. Next, we design two bias mitigation methods, namely embedding-based re-ranking (ER) and greedy substitution (GS), for deep recommender systems. ER is an in-processing mitigation method that equips (α, β)-fairness as a constraint to the objective function of the recommendation algorithm, while GS is a post-processing approach that accepts the biased recommendations as the input, and substitutes high-coverage items with low-coverage ones in the recommendations to satisfy (α, β)-fairness. We evaluate the performance of both mitigation algorithms on two real-world datasets and a set of state-of-the-art deep recommender systems. Our results demonstrate that both ER and GS outperform the existing minimum-coverage (MC) mitigation solutions [Koutsopoulos and Halkidi 2018; Patro et al. 2020] in terms of both fairness and accuracy of recommendations. Furthermore, ER delivers the best trade-off between fairness and recommendation accuracy among a set of alternative mitigation methods, including GS, the hybrid of ER and GS, and the existing MC solutions.",,,"['Xiuling Wang', 'Wendy Hui Wang']","['Stevens institute of technology', 'Stevens institute of technology']","['USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533104,Flipping the Script on Criminal Justice Risk Assessment: An actuarial model for assessing the risk the federal sentencing system poses to defendants,Fairness & Bias,"In the criminal justice system, algorithmic risk assessment instruments are used to predict the risk a defendant poses to society; examples include the risk of recidivating or the risk of failing to appear at future court dates. However, defendants are also at risk of harm from the criminal justice system. To date, there exists no risk assessment instrument that considers the risk the system poses to the individual. We develop a risk assessment instrument that “flips the script.” Using data about U.S. federal sentencing decisions, we build a risk assessment instrument that predicts the likelihood an individual will receive an especially lengthy sentence given factors that should be legally irrelevant to the sentencing decision. To do this, we develop a two-stage modeling approach. Our first-stage model is used to determine which sentences were “especially lengthy.” We then use a second-stage model to predict the defendant’s risk of receiving a sentence that is flagged as especially lengthy given factors that should be legally irrelevant. The factors that should be legally irrelevant include, for example, race, court location, and other socio-demographic information about the defendant. Our instrument achieves comparable predictive accuracy to risk assessment instruments used in pretrial and parole contexts. We discuss the limitations of our modeling approach and use the opportunity to highlight how traditional risk assessment instruments in various criminal justice settings also suffer from many of the same limitations and embedded value systems of their creators.","['risk assessment', 'criminal justice', 'heteroscedastic Bayesian additive regression trees', 'LASSO', 'two-stage model', 'perspective reversal']",['Applied computing → Law'],"['Mikaela Meyer', 'Aaron Horowitz', 'Erica Marshall', 'Kristian Lum']","['Department of Statistics & Data Science and Heinz College, Carnegie Mellon University', 'American Civil Liberties Union', 'Idaho Justice Project', 'Department of Computer and Information Science, University of Pennsylvania']","['USA', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533205,"Justice in Misinformation Detection Systems: An Analysis of Algorithms, Stakeholders, and Potential Harms",Fairness & Bias,"Faced with the scale and surge of misinformation on social media, many platforms and fact-checking organizations have turned to algorithms for automating key parts of misinformation detection pipelines. While offering a promising solution to the challenge of scale, the ethical and societal risks associated with algorithmic misinformation detection are not well-understood. In this paper, we employ and extend upon the notion of informational justice to develop a framework for explicating issues of justice relating to representation, participation, distribution of benefits and burdens, and credibility in the misinformation detection pipeline. Drawing on the framework: (1) we show how injustices materialize for stakeholders across three algorithmic stages in the pipeline; (2) we suggest empirical measures for assessing these injustices; and (3) we identify potential sources of these harms. This framework should help researchers, policymakers, and practitioners reason about potential harms or risks associated with these algorithms and provide conceptual guidance for the design of algorithmic fairness audits in this domain.","['algorithmic fairness', 'justice', 'misinformation detection', 'machine learning', 'informational justice']",[],"['Terrence Neumann', 'Maria De-Arteaga', 'Sina Fazelpour']","['University of Texas at Austin', 'University of Texas at Austin', 'Northeastern University']","['USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3534637,Data Governance in the Age of Large-Scale Data-Driven Language Technology,Privacy & Data Governance,"The recent emergence and adoption of Machine Learning technology, and specifically of Large Language Models, has drawn attention to the need for systematic and transparent management of language data. This work proposes an approach to global language data governance that attempts to organize data management amongst stakeholders, values, and rights. Our proposal is informed by prior work on distributed governance that accounts for human values and grounded by an international research collaboration that brings together researchers and practitioners from 60 countries. The framework we present is a multi-party international governance structure focused on language data, and incorporating technical and organizational tools needed to support its work.",,,"['Yacine Jernite', 'Huu Nguyen', 'Stella Biderman', 'Anna Rogers', 'Maraim Masoud', 'Valentin Danchev', 'Samson Tan', 'Alexandra Sasha Luccioni', 'Nishant Subramani', 'Isaac Johnson', 'Gerard Dupont', 'Jesse Dodge', 'Kyle Lo', 'Zeerak Talat', 'Dragomir Radev', 'Aaron Gokaslan', 'Somaieh Nikpoor', 'Peter Henderson', 'Rishi Bommasani', 'Margaret Mitchell']","['Hugging Face', 'Ontocord', 'EleutherAI', 'University of Copenhagen', 'Independent researcher', 'University of Essex', 'AWS AI Research & Education', 'Hugging Face', 'Allen Institute for Artificial Intelligence', 'Wikimedia', 'Independent', 'Allen Institute for Artificial Intelligence', 'Allen Institute for Artificial Intelligence', 'Simon Fraser University', 'Yale University', 'Cornell University', 'CAIDP', 'Stanford University', 'Stanford University', 'Hugging Face']","['USA', 'USA', 'USA', 'Denmark', 'Ireland', 'United Kingdom', 'USA', 'Canada', 'USA', 'USA', 'France', 'USA', 'USA', 'Canada', 'USA', 'USA', 'Canada', 'USA', 'USA', 'USA']"
2022,https://doi.org/10.1145/3531146.3533235,Model Explanations with Differential Privacy,Privacy & Data Governance,"Using machine learning models in critical decision-making processes has given rise to a call for algorithmic transparency. Model explanations, however, might leak information about the sensitive data used to train and explain the model, undermining data privacy. We focus on black-box feature-based model explanations, which locally approximate the model around the point of interest, using potentially sensitive data. We design differentially private local approximation mechanisms, and evaluate their effect on explanation quality. To protect training data, we use existing differentially private learning algorithms. However, to protect the privacy of data which is used during the local approximation, we design an adaptive differentially private algorithm, which finds the minimal privacy budget required to produce accurate explanations. Both empirically and analytically, we evaluate the impact of the randomness needed in differential privacy algorithms on the fidelity of model explanations.","['Differential Privacy', 'Model Explainations']",[],"['Neel Patel', 'Reza Shokri', 'Yair Zick']","['Viterbi School of engineering, University of Southern California', 'National University of Singapore', 'University of Massachusetts, Amherst']","['USA', 'Singapore', 'USA']"
2022,https://doi.org/10.1145/3531146.3533215,System Safety and Artificial Intelligence,Security,"This article formulates seven lessons for preventing harm in artificial intelligence (AI) systems based on insights from the field of system safety for software-based automation in safety-critical domains. New applications of AI across societal domains and public organizations and infrastructures come with new hazards, which lead to new forms of harm, both grave and pernicious. The text addresses the lack of consensus for diagnosing and eliminating new AI system hazards. For decades, the field of system safety has dealt with accidents and harm in safety-critical systems governed by varying degrees of software-based automation and decision-making. This field embraces the core assumption of systems and control that AI systems cannot be safeguarded by technical design choices on the model or algorithm alone, instead requiring an end-to-end hazard analysis and design frame that includes the context of use, impacted stakeholders and the formal and informal institutional environment in which the system operates. Safety and other values are then inherently socio-technical and emergent system properties that require design and control measures to instantiate these across the technical, social and institutional components of a system. This article honors system safety pioneer Nancy Leveson, by situating her core lessons for today’s AI system safety challenges [2]. For every lesson, concrete tools are offered for rethinking and reorganizing the safety management of AI systems, both in design and governance. This history tells us that effective AI safety management requires transdisciplinary approaches and a shared language that allows involvement of all levels of society.  The article is a non-archival contribution to FAccT 2022, and will be published as a chapter to The Oxford Handbook of AI Governance [1]. The full article is available as a pre-print on ArXiv via  https://arxiv.org/abs/2202.09292.",,,['Roel Dobbe'],"['Technology, Policy and Management, Delft University of Technology']",['Netherlands']
2022,https://doi.org/10.1145/3531146.3533213,Who Audits the Auditors? Recommendations from a field scan of the algorithmic auditing ecosystem,Security,"Algorithmic audits (or ‘AI audits’) are an increasingly popular mechanism for algorithmic accountability; however, they remain poorly defined. Without a clear understanding of audit practices, let alone widely used standards or regulatory guidance, claims that an AI product or system has been audited, whether by first-, second-, or third-party auditors, are difficult to verify and may potentially exacerbate, rather than mitigate, bias and harm. To address this knowledge gap, we provide the first comprehensive field scan of the AI audit ecosystem. We share a catalog of individuals (N=438) and organizations (N=189) who engage in algorithmic audits or whose work is directly relevant to algorithmic audits; conduct an anonymous survey of the group (N=152); and interview industry leaders (N=10). We identify emerging best practices as well as methods and tools that are becoming commonplace, and enumerate common barriers to leveraging algorithmic audits as effective accountability mechanisms. We outline policy recommendations to improve the quality and impact of these audits, and highlight proposals with wide support from algorithmic auditors as well as areas of debate. Our recommendations have implications for lawmakers, regulators, internal company policymakers, and standards-setting bodies, as well as for auditors. They are: 1) require the owners and operators of AI systems to engage in independent algorithmic audits against clearly defined standards; 2) notify individuals when they are subject to algorithmic decision-making systems; 3) mandate disclosure of key components of audit findings for peer review; 4) consider real-world harm in the audit process, including through standardized harm incident reporting and response mechanisms; 5) directly involve the stakeholders most likely to be harmed by AI systems in the algorithmic audit process; and 6) formalize evaluation and, potentially, accreditation of algorithmic auditors.",,,"['Sasha Costanza-Chock', 'Inioluwa Deborah Raji', 'Joy Buolamwini']","['Algorithmic Justice League', 'Algorithmic Justice League', 'Algorithmic Justice League']","['USA', 'USA', 'USA']"