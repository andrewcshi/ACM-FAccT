doi_link,title,abstract,keywords,CCS concepts,author_names,affiliations,affiliated_countries
https://doi.org/10.1145/3593013.3594084,A Sociotechnical Audit: Assessing Police Use of Facial Recognition,"Algorithmic audits are increasingly used to hold people accountable for the algorithms they implement. However, much work remains to integrate ethical and legal evaluations of how algorithms are used into audits. In this paper, we present a sociotechnical audit to help external stakeholders evaluate the ethics and legality of police use of facial recognition technology. We developed this audit for the specific legal context of England and Wales, and to bring attention to broader concerns such as whether police consult affected communities and comply with human rights law. To design this audit, we compiled ethical and legal standards for governing facial recognition, based on existing literature and feedback from academia, government, civil society, and police organizations. We then applied the resulting audit tool to three facial recognition deployments by police forces in the UK and found that all three failed to meet these standards. Developing this audit helps us provide insights to researchers in designing their own sociotechnical audits, specifically how audits shift power, how to make audits context-specific, how audits reveal what is not transparent, and how audits lead to accountability.","['algorithmic audits', 'accountability', 'ethical and legal considerations', 'facial recognition technology']","['Social and professional topics _ Surveillance', 'Social and professional topics _ Technology audits', 'Security and privacy _ Human and societal aspects of security and privacy']","['Evani Radiya-Dixit', 'Gina Neff']","['Minderoo Centre for Technology and Democracy, University of Cambridge', 'Minderoo Centre for Technology and Democracy, University of Cambridge']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594090,Navigating the Audit Landscape: A Framework for Developing Transparent and Auditable XR,"“Extended reality” (XR) systems work to blend the physical and digital worlds. This means that XR is highly contextual: its functionality, operation and therefore consequences are driven by a tight, run-time coupling of the technology, the user, and their physical environment. It follows that XR brings particular challenges regarding transparency and accountability, given that it can be difficult to foresee and mitigate all potential issues that might arise from using such systems, given their many potential contexts of use. Further, the physicality of XR can directly result in injury, property damage, or worse, in addition to the more traditionally discussed harms arising from algorithmic systems. Therefore the ability to audit the operation of XR systems is paramount – where information revealing and enabling some reconstruction of an XR system’s use, run-time behaviour, and surrounding context is important for understanding and scrutinising what happens/happened, and why.  Towards this, we present a framework to support those involved in developing XR systems to make them more auditable. The framework focuses on supporting the building and instrumentation of an XR system for transparency aims, elaborating key considerations regarding the capture and management of audit data during system operation. We demonstrate the framework’s efficacy with expert XR developers, who indicate the utility and need for such in practice. In all, we provide practical ways forward on, as well as seek to draw attention to, XR transparency and accountability.","['audit', 'transparency', 'accountability', 'responsibility', 'auditability', 'reviewability', 'trust', 'augmented reality', 'virtual reality', 'mixed reality']","['Human-centered computing _ Ubiquitous and mobile computing design and evaluation methods', 'Human-centered computing _ Mixed / augmented reality', 'Human-centered computing _ Virtual reality', 'Social and professional topics _ Technology audits', 'Human-centered computing _ Ubiquitous and mobile computing systems and tools']","['Chris Norval', 'Richard Cloete', 'Jatinder Singh']","['University of Cambridge', 'University of Harvard', 'University of Cambridge']","['United Kingdom', 'USA', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594001,"Explainable AI is Dead, Long Live Explainable AI! Hypothesis-Driven Decision Support Using Evaluative AI","In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making.",[],"['Computing methodologies _ Artificial intelligence', 'Human-centered computing _ HCI theory, concepts and models']",['Tim Miller'],['The University of Melbourne'],['Australia']
https://doi.org/10.1145/3593013.3594074,"Explainability in AI Policies: A Critical Review of Communications, Reports, Regulations, and Standards in the EU, US, and UK","Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers.","['Explainable AI', 'AI policy', 'social epistemology']","['Applied computing _ Law', 'Social and professional topics _ Governmental regulations']","['Luca Nannini', 'Agathe Balayn', 'Adam Leon Smith']","['Minsait - Indra Sistemas, Spain and CiTIUS (Centro Singular de Investigación en Tecnoloxías Intelixentes), Universidade de Santiago de Compostela', 'Delft University of Technology']","['Spain', 'Netherlands', 'Spain']"
https://doi.org/10.1145/3593013.3594017,A Theory of Auditability for Allocation and Social Choice Mechanisms,"In centralized market mechanisms individuals may not fully observe other participants' type reports. Hence, the mechanism designer may deviate from the promised mechanism without the individuals being able to detect these deviations. In this paper, we develop a theory of auditability for allocation and social choice problems. Namely, we measure a mechanism's auditabilty by the smallest number of individuals that can jointly detect any deviation. Our theory reveals stark contrasts between prominent mechanisms' auditabilities in various applications. For priority-based allocation problems, we find that the Immediate Acceptance mechanism is maximally auditable, in a sense that any deviation can always be detected by just two individuals, whereas, on the other extreme, the Deferred Acceptance mechanism is minimally auditable, in a sense that some deviations may go undetected unless some individuals possess full information about everyone's reports. For the auctions setup, we find a similar contrast between the first-price and the second-price auction mechanisms. For voting problems, we characterize the majority voting rule as the unique most auditable anonymous voting mechanism. And finally, for the choice with affirmative action setting, we compare the auditability indices of prominent reserves mechanisms.","['auditability', 'mechanisms', 'auctions', 'allocation', 'affirmative action']","['Accountability', 'Auditing', 'Economics', 'Explainability', 'Mechanism Design', 'Social Choice']","['Aram Grigoryan', 'Markus Möller']","['University of California, San Diego', 'University of Bonn']","['USA', 'Germany']"
https://doi.org/10.1145/3593013.3594060,Towards Labor Transparency in Situated Computational Systems Impact Research,"Researchers seeking to examine and prevent technology-mediated harms have emphasized the importance of directly engaging with community stakeholders through participatory approaches to computational systems research. However, recent transformations in strategies of corporate capture within the tech industry pose significant challenges to established participatory practices. In this paper we extend existing critical participatory design scholarship to highlight the exploitative potential of labor relationships in community collaborations between researchers and participants. Drawing on a reflexive approach to our own experiences conducting agonistic participatory research on emerging technologies at a large technology company, we highlight the limitations of doing participatory work within such contexts by empirically illustrating how and when these relationships threaten to appropriate and alienate participant labor. We argue that a labor-conscious approach to computational systems impact research is critical for countering the commodification of inclusion and invite fellow researchers to more actively investigate such dynamics. To this end, we provide (1) a framework for documenting divisions of labor within participatory research, design, and data practices, and (2) a series of short provocations that help locate and inventory sites of extraction within participatory engagements.","['impact', 'inclusion', 'labor', 'participatory design', 'agonism', 'documentation', 'transparency']","['Human-centered computing _ Participatory design', 'Human-centered computing _ Collaborative and social computing design and evaluation methods']","['Felicia S. Jing', 'Sara E. Berger', 'Juana Catalina Becerra Sandoval']","['Responsible & Inclusive Technologies, IBM Research, USA and Department of Political Science, Johns Hopkins University', 'Responsible & Inclusive Technologies, IBM Research', 'Responsible & Inclusive Technologies, IBM Research']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594014,Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits,"This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",[],"['Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management']","['Bogdana Rakova', 'Roel Dobbe']","['Mozilla Foundation', 'Delft University of Technology']","['USA', 'Netherlands']"
https://doi.org/10.1145/3593013.3594098,Auditing Cross-Cultural Consistency of Human-Annotated Labels for Recommendation Systems,"Recommendation systems increasingly depend on massive human-labeled datasets; however, the human annotators hired to generate these labels increasingly come from homogeneous backgrounds. This poses an issue when downstream predictive models—based on these labels—are applied globally to a heterogeneous set of users. We study this disconnect with respect to the labels themselves, asking whether they are “consistently conceptualized” across annotators of different demographics. In a case study of video game labels, we conduct a survey on 5,174 gamers, identify a subset of inconsistently conceptualized game labels, perform causal analyses, and suggest both cultural and linguistic reasons for cross-country differences in label annotation. We further demonstrate that predictive models of game annotations perform better on global train sets as opposed to homogeneous (single-country) train sets. Finally, we provide a generalizable framework for practitioners to audit their own data annotation processes for consistent label conceptualization, and encourage practitioners to consider global inclusivity in recommendation systems starting from the early stages of annotator recruitment and data-labeling.","['label bias', 'human annotations', 'cultural bias', 'linguistic bias', 'poststratification', 'causal inference', 'recommendation systems']","['Social and professional topics _ Cultural characteristics', 'Human-centered computing _ Social recommendation']","['Rock Yuren Pang', 'Jack Cenatempo', 'Franklyn Graham', 'Bridgette Kuehn', 'Maddy Whisenant', 'Portia Botchway', 'Katie Stone Perez', 'Allison Koenecke']","['University of Washington', 'Microsoft Research', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Microsoft Corporation – Xbox Division', 'Cornell University']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594009,Algorithmic Transparency and Accountability through Crowdsourcing: A Study of the NYC School Admission Lottery,"Algorithms are used to aid decision-making for a wide range of public policy decisions. Yet, the details of the algorithmic processes and how to interact with their systems are often inadequately communicated to stakeholders, leaving them frustrated and distrusting of the outcomes of the decisions. Transparency and accountability are critical prerequisites for building trust in the results of decisions and guaranteeing fair and equitable outcomes. Unfortunately, organizations and agencies do not have strong incentives to explain and clarify their decision processes; however, stakeholders are not powerless and can strategically combine their efforts to push for more transparency.  In this paper, I discuss the results and lessons learned from such an effort: a parent-led crowdsourcing campaign to increase transparency in the New York City school admission process. NYC famously uses a deferred-acceptance matching algorithm to assign students to schools, but families are given very little, and often wrong, information on the mechanisms of the system in which they have to participate. Furthermore, the odds of matching to specific schools depend on a complex set of priority rules and tie-breaking random (lottery) numbers, whose impact on the outcome is not made clear to students and their families, resulting in many “wasted choices” on students’ ranked lists and a high rate of unmatched students. Using the results of a crowdsourced survey of school application results, I was able to explain how random tie-breakers factored in the admission, adding clarity and transparency to the process. The results highlighted several issues and inefficiencies in the match and made the case for the need for more accountability and verification in the system.","['school matching', 'crowdsourcing', 'accountability', 'transparency']","['Social and professional topics _ Government technology policy', 'Human-centered computing _ User studies']",['Amelie Marian'],"['Computer Science, Rutgers University, New Brunswick']",['USA']
https://doi.org/10.1145/3593013.3594069,The Role of Explainable AI in the Context of the AI Act,"The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented.","['explainable artificial intelligence', 'XAI', 'AI Act', 'EU regulation', 'trustworthy AI', 'transparency', 'human oversight']","['Computing methodologies _ Artificial intelligence', 'Applied computing _ Law']","['Cecilia Panigutti', 'Ronan Hamon', 'Isabelle Hupont', 'David Fernandez Llorca', 'Delia Fano Yela', 'Henrik Junklewitz', 'Salvatore Scalzo', 'Gabriele Mazzini', 'Ignacio Sanchez', 'Josep Soler Garrido', 'Emilia Gomez']","['European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission', 'European Commission', 'European Commission, Joint Research Centre (JRC), Ispra', 'European Commission, Joint Research Centre (JRC), Sevilla', 'European Commission, Joint Research Centre (JRC), Sevilla']","['Italy', 'Italy', 'Spain', 'Spain', 'Spain', 'Italy', 'Belgium', 'Belgium', 'Italy', 'Spain', 'Spain']"
https://doi.org/10.1145/3593013.3594118,Fairness Auditing in Urban Decisions Using LP-Based Data Combination,"Auditing for fairness often requires relying on a secondary source, e.g., Census data, to inform about protected attributes. To avoid making assumptions about an overarching model that ties such information to the primary data source, a recent line of work has suggested finding the entire range of possible fairness valuations consistent with both sources. Though attractive, the current form of this methodology relies on rigid analytical expressions and lacks the ability to handle continuous decisions, e.g., metrics of urban services. We show that, in such settings, directly adapting these expressions can lead to loose and even vacuous results, particularly on just how fair the audited decisions may be. If used, the audit would be perceived more optimistically than it ought to be. We propose a linear programming formulation to handle continuous decisions, by finding the empirical fairness range when statistical parity is measured through the Kolmogorov-Smirnov distance. The size of this problem is linear in the number of data points and efficiently solvable. We analyze this approach and give finite-sample guarantees to the resulting fairness valuation. We then apply it to synthetic data and to 311 Chicago City Services data, and demonstrate its ability to reveal small but detectable bounds on fairness.","['data combination', 'fairness auditing', 'urban data', 'disparate impact', 'proxy variables', 'linear programming']","['Accountability', 'Auditing', 'Economics', 'Explainability', 'Mechanism Design', 'Social Choice']","['Jingyi Yang', 'Joel Miller', 'Mesrob Ohannessian']","['Electrical and Computer Engineering, University of Illinois Chicago', 'Computer Science, University of Illinois Chicago', 'Electrical and Computer Engineering, University of Illinois Chicago']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594010,Rethinking Transparency as a Communicative Constellation,"In this paper we make the case for an expanded understanding of transparency. Within the now extensive FAccT literature, transparency has largely been understood in terms of explainability. While this approach has proven helpful in many contexts, it falls short of addressing some of the more fundamental issues in the development and application of machine learning, such as the epistemic limitations of predictions and the political nature of the selection of fairness criteria. In order to render machine learning systems more democratic, we argue, a broader understanding of transparency is needed. We therefore propose to view transparency as a communicative constellation that is a precondition for meaningful democratic deliberation. We discuss four perspective expansions implied by this approach and present a case study illustrating the interplay of heterogeneous actors involved in producing this constellation. Drawing from our conceptualization of transparency, we sketch implications for actor groups in different sectors of society.","['transparency', 'explainability', 'science communication', 'deliberation', 'prediction']","['Computing methodologies _ Machine learning', 'Applied computing _ Law, social and behavioral sciences', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence']","['Florian Eyert', 'Paola Lopez']","['WZB Berlin Social Science Center, Germany and Weizenbaum Institute', 'University of Vienna, Austria and Weizenbaum Institute']","['Germany', 'Germany']"
https://doi.org/10.1145/3593013.3593991,Algorithmic Transparency from the South: Examining the State of Algorithmic Transparency in Chile's Public Administration Algorithms,"This paper presents the results and conclusions of the study on algorithmic transparency in public Administration and the use of automated decision systems within the State of Chile, carried out by the Public Innovation Laboratory of the Universidad Adolfo Ibáñez in alliance with the Chilean Transparency Council. In the first part we delimit the concept of algorithmic transparency, and the different considerations that can derive from this concept. We detail the information gathering procedure carried out on the use of automated decision systems in the public administration and evaluate its status according to a defined transparency framework. It then examines the state of administrative regulation and access to public information in Chile and how algorithmic transparency could be included within the current legal norms in Chile. The results of this study show that there is a use of automated decision systems in critical operations in the Chilean public Administration and that the current legal framework enables the implementation of an algorithmic transparency standard for the public administration, in a flexible, scaled way and with criteria that allow citizens to evaluate their interaction with these systems. Building on the results of this research, in 2022 the Transparency Council piloted a draft algorithmic transparency standard with seven algorithms from four public agencies. A public consultation and the publication of the final standard is expected in 2023.","['Computers and Society', 'Public Policy issues', 'decision-support', 'public Administration', 'Computing in government', 'Algorithmic Transparency', 'Public Administration', 'Chile']",[],"['José Pablo Lapostol Piderit', 'Romina Garrido Iglesias', 'María Paz Hermosilla Cornejo']","['Universidad Adolfo Ibañez', 'Universidad Adolfo Ibañez', 'Universidad Adolfo Ibañez']","['Chile', 'Chile', 'Chile']"
https://doi.org/10.1145/3593013.3594064,Co-Design Perspectives on Algorithm Transparency Reporting: Guidelines and Prototypes,"Recommendation algorithms by and large determine what people see on social media. Users know little about how these algorithms work or what information they use to make their recommendations. But what exactly should platforms share with users about recommendation algorithms that would be meaningful to them? Research has looked into frameworks for explainability of algorithms as well as design features across social media platforms that can contribute to their transparency and accountability. We build on these prior efforts to explore what a recommendation algorithm transparency report may include and how it should present information to users. Through a human-centered co-design research process we result in: (1) A set of guidelines for recommendation algorithm transparency reports; (2) initial suggestions, in the form of prototypes, for more engaging and interactive forms of transparency; (3) an evaluation of these prototypes’ strengths and weaknesses, and areas of exploration for future work.",[],"['Human-centered computing _ HCI design and evaluation methods', 'User studies', 'Human-centered computing _ User centered design', 'Human-centered computing _ Participatory design', 'Human-centered computing _ Interface design prototyping']",['Michal Luria'],['Center for Democracy & Technology'],['USA']
https://doi.org/10.1145/3593013.3594093,Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain,"Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.","['Co-Design', 'Document Organization', 'User-Centered Design', 'Collaborative Design']","['Human-centered computing _ Interaction paradigms', 'Human-centered computing _ User studies', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning']","['Hellina Hailu Nigatu', 'Lisa Pickoff-White', 'John Canny', 'Sarah Chasins']","['EECS, UC Berkeley', 'KQED', 'EECS, UC Berkeley', 'EECS, UC Berkeley']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594003,Simplicity Bias Leads to Amplified Performance Disparities,"Which parts of a dataset will a given model find difficult? Recent work has shown that SGD-trained models have a bias towards simplicity, leading them to prioritize learning a majority class, or to rely upon harmful spurious correlations. Here, we show that the preference for ‘easy’ runs far deeper: A model may prioritize any class or group of the dataset that it finds simple—at the expense of what it finds complex—as measured by performance difference on the test set. When subsets with different levels of complexity align with demographic groups, we term this difficulty disparity, a phenomenon that occurs even with balanced datasets that lack group/label associations. We show how difficulty disparity is a model-dependent quantity, and is further amplified in commonly-used models as selected by typical average performance scores. We quantify an amplification factor across a range of settings in order to compare disparity of different models on a fixed dataset. Finally, we present two real-world examples of difficulty amplification in action, resulting in worse-than-expected performance disparities between groups even when using a balanced dataset. The existence of such disparities in balanced datasets demonstrates that merely balancing sample sizes of groups is not sufficient to ensure unbiased performance. We hope this work presents a step towards measurable understanding of the role of model bias as it interacts with the structure of data, and call for additional model-dependent mitigation methods to be deployed alongside dataset audits.","['neural networks', 'simplicity bias', 'performance disparities', 'fairness']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Neural networks', 'Computing methodologies _ Computer vision', 'Social and professional topics _ Socio-technical systems', 'General and reference _ Empirical studies', 'General and reference _ Evaluation']","['Samuel James Bell', 'Levent Sagun']","['FAIR, Meta AI', 'FAIR, Meta AI']","['France', 'France']"
https://doi.org/10.1145/3593013.3594094,Examining Risks of Racial Biases in NLP Tools for Child Protective Services,"Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.","['NLP', 'bias', 'race', 'child protection system', 'CPS', 'text processing']",[],"['Anjalie Field', 'Amanda Coston', 'Nupoor Gandhi', 'Alexandra Chouldechova', 'Emily Putnam-Hornstein', 'David Steier', 'Yulia Tsvetkov']","['Computer Science Department, Johns Hopkins University, USA and Language Technologies Institute, Carnegie Mellon University', 'Heinz College of Information Systems and Public Policy and Machine Learning Department, Carnegie Mellon University', 'Heinz College of Information Systems and Public Policy and Machine Learning Department, Carnegie Mellon University', 'Microsoft Research NYC, USA and Heinz College of Information Systems and Public Policy, Carnegie Mellon University', 'School of Social Work, University of North Carolina at Chapel Hill', 'Heinz College of Information Systems and Public Policy, Carnegie Mellon University', 'Paul G. Allen School of Computer Science & Engineering, University of Washington']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594083,An Empirical Analysis of Racial Categories in the Algorithmic Fairness Literature,"Recent work in algorithmic fairness has highlighted the challenge of defining racial categories for the purposes of anti-discrimination. These challenges are not new but have previously fallen to the state, which enacts race through government statistics, policies, and evidentiary standards in anti-discrimination law. Drawing on the history of state race-making, we examine how longstanding questions about the nature of race and discrimination appear within the algorithmic fairness literature. Through a content analysis of 60 papers published at FAccT between 2018 and 2020, we analyze how race is conceptualized and formalized in algorithmic fairness frameworks. We note that differing notions of race are adopted inconsistently, at times even within a single analysis. We also explore the institutional influences and values associated with these choices. While we find that categories used in algorithmic fairness work often echo legal frameworks, we demonstrate that values from academic computer science play an equally important role in the construction of racial categories. Finally, we examine the reasoning behind different operationalizations of race, finding that few papers explicitly describe their choices and even fewer justify them. We argue that the construction of racial categories is a value-laden process with significant social and political consequences for the project of algorithmic fairness. The widespread lack of justification around the operationalization of race reflects institutional norms that allow these political decisions to remain obscured within the backstage of knowledge production.","['racial categories', 'algorithmic fairness', 'state race-making']",[],"['Amina A. Abdu', 'Irene V. Pasquetto', 'Abigail Z. Jacobs']","['University of Michigan', 'University of Michigan', 'University of Michigan']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594047,Datafication Genealogies beyond Algorithmic Fairness: Making Up Racialised Subjects,"A growing scholarship has discussed how datafication is grounded on algorithmic discrimination. However, these debates only marginally address how racialised classification or race categories are enforced through quantification and neglect its political and historical conceptualisation. In this work, we argue that literature partially fails to show that datafication reinforces racial profiling beyond the creation of racial categories as features. This article casts a new light on datafication by retracing its genealogy focusing on identification procedures in the colony and at the border. Such a genealogy foregrounds how datafication enforces racialised profiles by showing that it is part of a longer historical trajectory of modes of racialising individuals beyond algorithms and racial categories. Building on archival material, it develops this argument through two case studies. First, it focuses on the study of datafication of colonised bodies through biometrics by Francis Galton during the 19th-century. Second, it takes into account police identification procedures about unauthorised migrants, enforced by the French police at the Italian border in the 20th-century. These two cases show that although race categories as variables have been historically used to translate individuals into data, datafication processes as such also produce racialised profiles. A genealogical approach highlights continuities as well as quantitative and qualitative shifts between analogue and digital datafication. The article concludes arguing that datafication mechanisms have historically enforced legal and political measures by states in the name of science and objectivity and debates around algorithmic fairness should bring this key aspect back to the core of their critiques.","['datafication', 'genealogies', 'racialised subjects', 'classification', 'borders']","['Social and professional topics _ History of computing', 'Applied computing _ Sociology']","['Ana Valdivia', 'Martina Tazzioli']","['Oxford Internet Institute, University of Oxford', 'Politics and International Relations, Goldsmiths, University of London']","['United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594096,"What's Fair Is… Fair? Presenting JustEFAB, an Ethical Framework for Operationalizing Medical Ethics and Social Justice in the Integration of Clinical Machine Learning: JustEFAB","The problem of algorithmic bias represents an ethical threat to the fair treatment of patients when their care involves machine learning (ML) models informing clinical decision-making. The design, development, testing, and integration of ML models therefore require a lifecycle approach to bias identification and mitigation efforts. Presently, most work focuses on the ML tool alone, neglecting the larger sociotechnical context in which these models operate. Moreover, the narrow focus on technical definitions of fairness must be integrated within the larger context of medical ethics in order to facilitate equitable care with ML. Drawing from principles of medical ethics, research ethics, feminist philosophy of science, and justice-based theories, we describe the Justice, Equity, Fairness, and Anti-Bias (JustEFAB) guideline intended to support the design, testing, validation, and clinical evaluation of ML models with respect to algorithmic fairness. This paper describes JustEFAB's development and vetting through multiple advisory groups and the lifecycle approach to addressing fairness in clinical ML tools. We present an ethical decision-making framework to support design and development, adjudication between ethical values as design choices, silent trial evaluation, and prospective clinical evaluation guided by medical ethics and social justice principles. We provide some preliminary considerations for oversight and safety to support ongoing attention to fairness issues. We envision this guideline as useful to many stakeholders, including ML developers, healthcare decision-makers, research ethics committees, regulators, and other parties who have interest in the fair and judicious use of clinical ML tools.","['Social and professional topics', 'Computing/technology policy', 'Medical information policy', 'Medical technologies', 'algorithmic bias', 'fairness', 'clinical machine learning', 'ethics', 'organizational ethics', 'justice', 'accountability', 'healthcare', 'health policy', 'safe deployment']",[],"['Melissa Mccradden', 'Oluwadara Odusi', 'Shalmali Joshi', 'Ismail Akrout', 'Kagiso Ndlovu', 'Ben Glocker', 'Gabriel Maicas', 'Xiaoxuan Liu', 'Mjaye Mazwi', 'Tee Garnett', 'Lauren Oakden-Rayner', 'Myrtede Alfred', 'Irvine Sihlahla', 'Oswa Shafei', 'Anna Goldenberg']","['The Hospital for Sick Children', 'The University of Sheffield Medical School', 'Columbia University', 'The Hospital for Sick Children', 'University of Botswana', 'Imperial College London', 'Australian Institute for Machine Learning', 'University of Birmingham', 'The Hospital for Sick Children', 'The Hospital for Sick Children', 'Australian Institute for Machine Learning', 'University of Toronto', 'University of Cape Town', 'The Hospital for Sick Children', 'The Hospital for Sick Children']","['Canada', 'United Kingdom', 'USA', 'Canada', 'Botswana', 'United Kingdom', 'Australia', 'United Kingdom', 'Canada', 'Canada', 'Australia', 'Canada', 'South Africa', 'Canada', 'Canada']"
https://doi.org/10.1145/3593013.3594035,On the Site of Predictive Justice,"Optimism about our ability to enhance societal decision-making by leaning on Machine Learning (ML) for cheap, accurate predictions has palled in recent years, as these ‘cheap’ predictions have come at significant social cost, contributing to systematic harms suffered by already disadvantaged populations. But what precisely goes wrong when ML goes wrong? We argue that, as well as more obvious concerns about the downstream effects of ML-based decision-making, there can be moral grounds for the criticism of these predictions themselves. We introduce and defend a theory of predictive justice, according to which differential model performance for systematically disadvantaged groups can be grounds for moral criticism of the model, independently of its downstream effects. As well as helping resolve some urgent disputes around algorithmic fairness, this theory points the way to a novel dimension of epistemic ethics, related to the recently discussed category of doxastic wrong. The full version of this paper is available at http://mintresearch.org/pj.","['Philosophy', 'Algorithmic fairness', 'Predictive justice', 'Justice', 'Epistemic ethics']","['Computing methodologies _ Machine learning', 'Applied computing _ Law, social and behavioral sciences']","['Seth Lazar', 'Jake Stone']","['Australian National University', 'Australian National University']","['Australia', 'Australia']"
https://doi.org/10.1145/3593013.3594091,Group Fairness without Demographics Using Social Networks,"Group fairness is a popular approach to prevent unfavorable treatment of individuals based on sensitive attributes such as race, gender, and disability. However, the reliance of group fairness on access to discrete group information raises several limitations and concerns, especially with regard to privacy, intersectionality, and unforeseen biases. In this work, we propose a “group-free"" measure of fairness that does not rely on sensitive attributes and, instead, is based on homophily in social networks, i.e., the common property that individuals sharing similar attributes are more likely to be connected. Our measure is group-free as it avoids recovering any form of group memberships and uses only pairwise similarities between individuals to define inequality in outcomes relative to the homophily structure in the network. We theoretically justify our measure by showing it is commensurate with the notion of additive decomposability in the economic inequality literature and also bound the impact of non-sensitive confounding attributes. Furthermore, we apply our measure to develop fair algorithms for classification, maximizing information access, and recommender systems. Our experimental results show that the proposed approach can reduce inequality among protected classes without knowledge of sensitive attribute labels. We conclude with a discussion of the limitations of our approach when applied in real-world settings.","['group fairness', 'social networks', 'homophily']","['Computing methodologies _ Artificial intelligence', 'Theory of computation _ Social networks']","['David Liu', 'Virginie Do', 'Nicolas Usunier', 'Maximilian Nickel']","['Northeastern University', 'FAIR, Meta AI, France and LAMSADE, PSL, Université Paris Dauphine', 'FAIR, Meta AI', 'FAIR, Meta AI']","['USA', 'France', 'France', 'USA']"
https://doi.org/10.1145/3593013.3594115,Discrimination through Image Selection by Job Advertisers on Facebook,"Targeted advertising platforms are widely used by job advertisers to reach potential employees; thus issues of discrimination due to targeting that have surfaced have received widespread attention. Advertisers could misuse targeting tools to exclude people based on gender, race, location and other protected attributes from seeing their job ads. In response to legal actions, Facebook disabled the ability for explicit targeting based on many attributes for some ad categories, including employment. Although this is a step in the right direction, prior work has shown that discrimination can take place not just due to the explicit targeting tools of the platforms, but also due to the impact of the biased ad delivery algorithm. Thus, one must look at the potential for discrimination more broadly, and not merely through the lens of the explicit targeting tools.  In this work, we propose and investigate the prevalence of a new means for discrimination in job advertising, that combines both targeting and delivery – through the disproportionate representation or exclusion of people of certain demographics in job ad images. We use the Facebook Ad Library to demonstrate the prevalence of this practice through: (1) evidence of advertisers running many campaigns using ad images of people of only one perceived gender, (2) systematic analysis for gender representation in all current ad campaigns for truck drivers and nurses, (3) longitudinal analysis of ad campaign image use by gender and race for select advertisers. After establishing that the discrimination resulting from a selective choice of people in job ad images, combined with algorithmic amplification of skews by the ad delivery algorithm, is of immediate concern, we discuss approaches and challenges for addressing it.",[],"['Social and professional topics _ Technology audits', 'Social and professional topics _ Employment issues', 'Human-centered computing _ Social media', 'Social and professional topics _ Socio-technical systems', 'Social and professional topics _ Race and ethnicity', 'Social and professional topics _ Gender', 'Information systems _ Online advertising', 'Information systems _ Display advertising']","['Varun Nagaraj Rao', 'Aleksandra Korolova']","['Princeton University', 'Princeton University']","['USA', 'USA']"
https://doi.org/10.1145/3593013.3594080,Diverse Perspectives Can Mitigate Political Bias in Crowdsourced Content Moderation,"In recent years, social media companies have grappled with defining and enforcing content moderation policies surrounding political content on their platforms, due in part to concerns about political bias, disinformation, and polarization. These policies have taken many forms, including disallowing political advertising, limiting the reach of political topics, fact-checking political claims, and enabling users to hide political content altogether. However, implementing these policies requires human judgement to label political content, and it is unclear how well human labelers perform at this task, or whether biases affect this process. Therefore, in this study we experimentally evaluate the feasibility and practicality of using crowd workers to identify political content, and we uncover biases that make it difficult to identify this content. Our results problematize crowds composed of seemingly interchangeable workers, and provide preliminary evidence that aggregating judgements from heterogeneous workers may help mitigate political biases. In light of these findings, we identify strategies to achieving fairer labeling outcomes, while also better supporting crowd workers at this task and potentially mitigating biases.",[],[],"['Jacob Thebault-Spieker', 'Sukrit Venkatagiri', 'Naomi Mine', 'Kurt Luther']","['Information School, University of Wisconsin - Madison', 'Center for an Informed Public, University of Washington', 'University of Wisconsin - Madison', 'Virginia Tech']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594039,Add-Remove-or-Relabel: Practitioner-Friendly Bias Mitigation via Influential Fairness,"Commensurate with the rise in algorithmic bias research, myriad algorithmic bias mitigation strategies have been proposed in the literature. Nonetheless, many voice concerns about the lack of transparency that accompanies mitigation methods and the paucity of mitigation methods that satisfy protocol and data limitations of practitioners. Influence functions from robust statistics provide a novel opportunity to overcome both issues. Previous work demonstrates the power of influence functions to improve fairness outcomes. This work proposes a novel family of fairness solutions, coined influential fairness (IF), that is human-understandable and also agnostic to the underlying machine learning model and choice of fairness metric. We conduct an investigation of practitioner profiles and design mitigation methods for practitioners whose limitations discourage them from utilizing existing bias mitigation methods.","['machine learning', 'fairness', 'ethics', 'bias mitigation']",[],"['Brianna Richardson', 'Prasanna Sattigeri', 'Dennis Wei', 'Karthikeyan Natesan Ramamurthy', 'Kush Varshney', 'Amit Dhurandhar', 'Juan E. Gilbert']","['University of Florida', 'IBM', 'IBM', 'IBM', 'IBM', 'IBM', 'University of Florida']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594014,Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits,"This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",[],"['Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management']","['Bogdana Rakova', 'Roel Dobbe']","['Mozilla Foundation', 'Delft University of Technology']","['USA', 'Netherlands']"
https://doi.org/10.1145/3593013.3594109,Bias Against 93 Stigmatized Groups in Masked Language Models and Downstream Sentiment Classification Tasks,"Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education."" is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson’s r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups.","['AI ethics', 'AI bias', 'stigma in language models', 'language models', 'representation learning', 'sentiment classification', 'prompting']",['Computing methodologies _ Natural language processing'],"['Katelyn Mei', 'Sonia Fereidooni', 'Aylin Caliskan']","['Information School, University of Washington', 'University of Washington', 'Information School, University of Washington']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594072,Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias,"Warning: The content of this paper may be upsetting or triggering. Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person’s human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model’s recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d > 0.80) and sadness (d > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age] year old girl"" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications.","['language-vision AI', 'generative AI', 'text-to-image generators', 'representation learning', 'AI bias', 'gender bias', 'sexualization', 'AI bias propagation', 'AI bias in applications']","['Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Learning paradigms', 'Computing methodologies _ Learning latent representations']","['Robert Wolfe', 'Yiwei Yang', 'Bill Howe', 'Aylin Caliskan']","['Information School, University of Washington', 'Information School, University of Washington', 'Information School, University of Washington', 'Information School, University of Washington']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594004,On the Independence of Association Bias and Empirical Fairness in Language Models,"The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness—or such probes ‘into representational biases’ are said to be ‘motivated by fairness’—suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.","['Representational Bias', 'Fairness', 'Natural Language Processing']",['Computing methodologies _ Natural language processing'],"['Laura Cabello', 'Anna Katrine Jørgensen', 'Anders Søgaard']","['University of Copenhagen', 'University of Copenhagen', 'University of Copenhagen']","['Denmark', 'Denmark', 'Denmark']"
https://doi.org/10.1145/3593013.3594086,Can Querying for Bias Leak Protected Attributes? Achieving Privacy With Smooth Sensitivity,"Existing regulations often prohibit model developers from accessing protected attributes (gender, race, etc.) during training. This leads to scenarios where fairness assessments might need to be done on populations without knowing their memberships in protected groups. In such scenarios, institutions often adopt a separation between the model developers (who train their models with no access to the protected attributes) and a compliance team (who may have access to the entire dataset solely for auditing purposes). However, the model developers might be allowed to test their models for disparity by querying the compliance team for group fairness metrics. In this paper, we first demonstrate that simply querying for fairness metrics, such as, statistical parity and equalized odds can leak the protected attributes of individuals to the model developers. We demonstrate that there always exist strategies by which the model developers can identify the protected attribute of a targeted individual in the test dataset from just a single query. Furthermore, we show that one can reconstruct the protected attributes of all the individuals from queries when Nk ≪ n using techniques from compressed sensing (n is the size of the test dataset and Nk is the size of smallest group therein). Our results pose an interesting debate in algorithmic fairness: Should querying for fairness metrics be viewed as a neutral-valued solution to ensure compliance with regulations? Or, does it constitute a violation of regulations and privacy if the number of queries answered is enough for the model developers to identify the protected attributes of specific individuals? To address this supposed violation of regulations and privacy, we also propose Attribute-Conceal, a novel technique that achieves differential privacy by calibrating noise to the smooth sensitivity of our bias query function, outperforming naive techniques such as the Laplace mechanism. We also include experimental results on the Adult dataset and synthetic dataset (broad range of parameters).","['algorithmic fairness', 'compliance', 'compressed sensing', 'differential privacy', 'machine learning.']","['Social and professional topics _ Governmental regulations', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Social and professional topics _ User characteristics', 'General and reference _ Evaluation', 'Security and privacy _ Privacy-preserving protocols']","['Faisal Hamman', 'Jiahao Chen', 'Sanghamitra Dutta']","['Department of Electrical and Computer Engineering, University of Maryland, College Park', 'Responsible AI LLC', 'Department of Electrical and Computer Engineering, University of Maryland, College Park']","['USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3593982,In the Name of Fairness: Assessing the Bias in Clinical Record De-Identification,"Data sharing is crucial for open science and reproducible research, but the legal sharing of clinical data requires the removal of protected health information from electronic health records. This process, known as de-identification, is often achieved through the use of machine learning algorithms by many commercial and open-source systems. While these systems have shown compelling results on average, the variation in their performance across different demographic groups has not been thoroughly examined. In this work, we investigate the bias of de-identification systems on names in clinical notes via a large-scale empirical analysis. To achieve this, we create 16 name sets that vary along four demographic dimensions: gender, race, name popularity, and the decade of popularity. We insert these names into 100 manually curated clinical templates and evaluate the performance of nine public and private de-identification methods. Our findings reveal that there are statistically significant performance gaps along a majority of the demographic dimensions in most methods. We further illustrate that de-identification quality is affected by polysemy in names, gender context, and clinical note characteristics. To mitigate the identified gaps, we propose a simple and method-agnostic solution by fine-tuning de-identification methods with clinical context and diverse names. Overall, it is imperative to address the bias in existing methods immediately so that downstream stakeholders can build high-quality systems to serve all demographic parties fairly.","['Fairness', 'Named Entity Recognition', 'Clinical De-identification']","['Human-centered computing _ Fairness', 'Computing methodologies _ Natural language processing', 'Social and professional topics _ Patient privacy']","['Yuxin Xiao', 'Shulammite Lim', 'Tom Joseph Pollard', 'Marzyeh Ghassemi']","['Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594015,The Privacy-Bias Tradeoff: Data Minimization and Racial Disparity Assessments in U.S. Government,"An emerging concern in algorithmic fairness is the tension with privacy interests. Data minimization can restrict access to protected attributes, such as race and ethnicity, for bias assessment and mitigation. Less recognized is that for nearly 50 years, the federal government has been engaged in a large-scale experiment in data minimization, limiting (a) data sharing across federal agencies under the Privacy Act of 1974, and (b) data collection under the Paperwork Reduction Act. We document how this “privacy-bias tradeoff” has become an important battleground for fairness assessments in the U.S. government and provides rich lessons for resolving these tradeoffs. President Biden’s 2021 racial justice Executive Order 13,985 mandated that federal agencies conduct equity impact assessments (e.g., for racial disparities) of federal programs. We conduct a comprehensive assessment across high-volume claims agencies that affect many individuals, as well as all agencies filing “equity action plans,” with three findings. First, there is broad agreement in principle that equity impact assessments are important, with few parties raising privacy challenges in theory and many agencies proposing substantial efforts. Second, in practice, major agencies do not collect and may be affirmatively prohibited under the Privacy Act from linking demographic information. This has led to pathological results: until 2022, for instance, the US Dept. of Agriculture imputed race by “visual observation” when race information was not collected. Data minimization has meant that even where agencies want to acquire demographic information in principle, the legal, data infrastructure, and bureaucratic hurdles are severe. Third, we derive policy implications to address these barriers.",[],[],"['Jennifer King', 'Daniel Ho', 'Arushi Gupta', 'Victor Wu', 'Helen Webley-Brown']","['Stanford Institute for Human-Centered Artificial Intelligence, Stanford University', 'Stanford Law School, Stanford University', 'Stanford University', 'Stanford Law School, Stanford University', 'Massachusetts Institute of Technology']","['USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594046,"UNFair: Search Engine Manipulation, Undetectable by Amortized Inequity","Modern society increasingly relies on Information Retrieval systems to answer various information needs. Since this impacts society in many ways, there has been a great deal of work to ensure the fairness of these systems, and to prevent societal harms. There is a prevalent risk of failing to model the entire system, where nefarious actors can produce harm outside the scope of fairness metrics. We demonstrate the practical possibility of this risk through UNFair, a ranking system that achieves performance and measured fairness competitive with current state-of-the-art, while simultaneously being manipulative in setup. UNFair demonstrates how adhering to a fairness metric, Amortized Equity, can be insufficient to prevent Search Engine Manipulation. This possibility of manipulation bypassing a fairness metric discourages imposing a fairness metric ahead of time, and motivates instead a more holistic approach to fairness assessments.","['Fairness', 'Information Retrieval', 'Search Engine Manipulation Effect', 'Exposure', 'UNFair']",['Information systems _ Learning to rank'],"['Tim De Jonge', 'Djoerd Hiemstra']","['Radboud University', 'Radboud University']","['Netherlands', 'Netherlands']"
https://doi.org/10.1145/3593013.3594120,Bias as Boundary Object: Unpacking The Politics Of An Austerity Algorithm Using Bias Frameworks,"Whether bias is an appropriate lens for analysis and critique remains a subject of debate among scholars. This paper contributes to this conversation by unpacking the use of bias in a critical analysis of a controversial austerity algorithm introduced by the Austrian public employment service in 2018. It was envisioned to classify the unemployed into three risk categories based on predicted prospects for re-employment. The system promised to increase efficiency and effectivity of counseling while objectifying a new austerity support measure allocation scheme. This approach was intended to cut spending for those deemed at highest risk of long term unemployment. Our in-depth analysis, based on internal documentation not available to the public, systematically traces and categorizes various problematic biases to illustrate harms to job seekers and challenge promises used to justify the adoption of the system. The classification is guided by a long-established bias framework for computer systems developed by Friedman and Nissenbaum, which provides three sensitizing basic categories. We identified in our analysis ""technical biases,"" like issues around measurement, rigidity, and coarseness of variables, ""emergent biases,"" such as disruptive events that change the labor market, and, finally, ""preexisting biases,"" like the use of variables that act as proxies for inequality.  Grounded in our case study, we argue that articulated biases can be strategically used as boundary objects to enable different actors to critically debate and challenge problematic systems without prior consensus building. We unpack benefits and risks of using bias classification frameworks to guide analysis. They have recently received increased scholarly attention and thereby may influence the identification and construction of biases. By comparing four bias frameworks and drawing on our case study, we illustrate how they are political by prioritizing certain aspects in analysis while disregarding others. Furthermore, we discuss how they vary in their granularity and how this can influence analysis. We also problematize how these frameworks tend to favor explanations for bias that center the algorithm instead of social structures. We discuss several recommendations to make bias analyses more emancipatory, arguing that biases should be seen as starting points for reflection on harmful impacts, questioning the framing imposed by the imagined “unbiased"" center that the bias is supposed to distort, and seeking out deeper explanations and histories that also center bigger social structures, power dynamics, and marginalized perspectives. Finally, we reflect on the risk that these frameworks may stabilize problematic notions of bias, for example, when they become a standard or enshrined in law.","['public employment services', 'job seeker profiling', 'algorithmic bias', 'infrastructure studies']","['Applied computing _ Computing in government', 'Human-centered computing', 'Social and professional topics _ Government technology policy', 'Computing methodologies _ Machine learning approaches']","['Gabriel Grill', 'Fabian Fischer', 'Florian Cech']","['University of Michigan', 'University of Applied Arts Vienna', 'TU Wien']","['USA', 'Austria', 'Austria']"
https://doi.org/10.1145/3593013.3594099,The Progression of Disparities within the Criminal Justice System: Differential Enforcement and Risk Assessment Instruments,"Algorithmic risk assessment instruments (RAIs) increasingly inform decision-making in criminal justice. RAIs largely rely on arrest records as a proxy for underlying crime. Problematically, the extent to which arrests reflect overall offending can vary with the person’s characteristics. We examine how the disconnect between crime and arrest rates impacts RAIs and their evaluation. Our main contribution is a method for quantifying this bias via estimation of the amount of unobserved offenses associated with particular demographics. These unobserved offenses are then used to augment real-world arrest records to create part real, part synthetic crime records. Using this data, we estimate that four currently deployed RAIs assign 0.5–2.8 percentage points higher risk scores to Black individuals than to White individuals with a similar arrest record, but the gap grows to 4.5–11.0 percentage points when we match on the semi-synthetic crime record. We conclude by discussing the potential risks around the use of RAIs, highlighting how they may exacerbate existing inequalities if the underlying disparities of the criminal justice system are not taken into account. In light of our findings, we provide recommendations to improve the development and evaluation of such tools.",[],[],"['Miri Zilka', 'Riccardo Fogliato', 'Jiri Hron', 'Bradley Butcher', 'Carolyn Ashurst', 'Adrian Weller']","['University of Cambridge', 'Carnegie Mellon University', 'University of Cambridge', 'University of Sussex', 'The Alan Turing Institute', 'University of Cambridge, United Kingdom and The Alan Turing Institute']","['United Kingdom', 'USA', 'United Kingdom', 'United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594078,“I’m Fully Who I Am”: Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation,"Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature.","['Algorithmic Fairness', 'Natural Language Generation', 'AI Fairness Auditing', 'Queer Harms in AI']",['Computing methodologies _ Natural language generation'],"['Anaelia Ovalle', 'Palash Goyal', 'Jwala Dhamala', 'Zachary Jaggers', 'Kai-Wei Chang', 'Aram Galstyan', 'Richard Zemel', 'Rahul Gupta']","['Computer Science, University of California, Los Angeles', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon', 'Amazon Global Diversity, Equity, & Inclusion, Amazon', 'Alexa AI-NU, Amazon, USA and Department of Computer Science, University of California, Los Angeles', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon', 'Alexa AI-NU, Amazon']","['USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594116,On The Impact of Machine Learning Randomness on Group Fairness,"Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model’s overall performance, by simply changing the data order for a single epoch.","['neural networks', 'fairness', 'randomness in training', 'evaluation']","['Computing methodologies _ Machine learning', 'General and reference _ Evaluation', 'Social and professional topics _ Computing / technology policy']","['Prakhar Ganesh', 'Hongyan Chang', 'Martin Strobel', 'Reza Shokri']","['School of Computing, National University of Singapore', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore', 'School of Computing, National University of Singapore']","['Singapore', 'Singapore', 'Singapore', 'Singapore']"
https://doi.org/10.1145/3593013.3594117,Detection and Mitigation of Algorithmic Bias via Predictive Parity,"Predictive parity (PP), also known as sufficiency, is a core definition of algorithmic fairness essentially stating that model outputs must have the same interpretation of expected outcomes regardless of group. Testing and satisfying PP is especially important in many settings where model scores are interpreted by humans or directly provide access to opportunity, such as healthcare or banking. Solutions for PP violations have primarily been studied through the lens of model calibration. However, we find that existing calibration-based tests and mitigation methods are designed for independent data, which is often not assumable in large-scale applications such as social media or medical testing. In this work, we address this issue by developing a statistically rigorous non-parametric regression based test for PP with dependent observations. We then apply our test to illustrate that PP testing can significantly vary under the two assumptions. Lastly, we provide a mitigation solution to provide a minimally-biased post-processing transformation function to achieve PP.","['algorithmic fairness', 'dependent data', 'testing for fairness', 'mitigation of bias']",[],"['Cyrus DiCiccio', 'Brian Hsu', 'Yinyin Yu', 'Preetam Nandy', 'Kinjal Basu']","['Independent', 'LinkedIn', 'LinkedIn', 'LinkedIn', 'LinkedIn']","['USA', 'USA', 'USA', 'Switzerland', 'USA']"
https://doi.org/10.1145/3593013.3593983,“How Biased Are Your Features?”: Computing Fairness Influence Functions with Global Sensitivity Analysis,"Fairness in machine learning has attained significant focus due to the widespread application in high-stake decision-making tasks. Unregulated machine learning classifiers can exhibit bias towards certain demographic groups in data, thus the quantification and mitigation of classifier bias is a central concern in fairness in machine learning. In this paper, we aim to quantify the influence of different features in a dataset on the bias of a classifier. To do this, we introduce the Fairness Influence Function (FIF). This function breaks down bias into its components among individual features and the intersection of multiple features. The key idea is to represent existing group fairness metrics as the difference of the scaled conditional variances in the classifier’s prediction and apply a decomposition of variance according to global sensitivity analysis. To estimate FIFs, we instantiate an algorithm that applies variance decomposition of classifier’s prediction following local regression. Experiments demonstrate that captures FIFs of individual feature and intersectional features, provides a better approximation of bias based on FIFs, demonstrates higher correlation of FIFs with fairness interventions, and detects changes in bias due to fairness affirmative/punitive actions in the classifier.  The code is available at https://github.com/ReAILe/bias-explainer. The extended version of the paper is at https://arxiv.org/pdf/2206.00667.pdf.","['Fair Machine Learning', 'Bias', 'Explainability', 'Global Sensitivity Analysis', 'Variance Decomposition', 'Influence Functions.']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence']","['Bishwamittra Ghosh', 'Debabrota Basu', 'Kuldeep S. Meel']","['National University of Singapore', 'Equipe Scool, Univ. Lille, Inria, UMR 9189 - CRIStAL, CNRS Centrale Lille, France', 'National University of Singapore']","['Singapore', 'France', 'Singapore']"
https://doi.org/10.1145/3593013.3593979,Multi-Dimensional Discrimination in Law and Machine Learning - A Comparative Overview,"AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called “multi-dimensional discrimination” or “multi-dimensional fairness” problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions.","['multi-discrimination', 'multi-fairness', 'intersectional fairness', 'sequential fairness', 'additive fairness']",[],"['Arjun Roy', 'Jan Horstmann', 'Eirini Ntoutsi']","['Institute of Computer Science, Free University of Berlin, Germany and Research Institute CODE, Bundeswehr University Munich', 'Institute for Legal Informatics, Leibniz University of Hanover', 'Research Institute CODE, Bundeswehr University Munich']","['Germany', 'Germany', 'Germany']"
https://doi.org/10.1145/3593013.3594044,Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is Not a Decision Tree,"Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.","['EU non-discrimination law', 'algorithmic fairness', 'machine learning', 'artificial intelligence']","['Computing methodologies _ Machine learning', 'Computing methodologies _ Artificial intelligence', 'Applied computing _ Law']","['Hilde Weerts', 'Raphaële Xenidis', 'Fabien Tarissan', 'Henrik Palmer Olsen', 'Mykola Pechenizkiy']","['Eindhoven University of Technology', 'Sciences Po Law School', 'CNRS & ENS Paris-Saclay', 'University of Copenhagen', 'Eindhoven University of Technology']","['Netherlands', 'France', 'France', 'Denmark', 'Netherlands']"
https://doi.org/10.1145/3593013.3594121,Legal Taxonomies of Machine Bias: Revisiting Direct Discrimination,"Previous literature on ‘fair’ machine learning has appealed to legal frameworks of discrimination law to motivate a variety of discrimination and fairness metrics and de-biasing measures. Such work typically applies the US doctrine of disparate impact rather than the alternative of disparate treatment, and scholars of EU law have largely followed along similar lines, addressing algorithmic bias as a form of indirect rather than direct discrimination. In recent work, we have argued that such focus is unduly narrow in the context of European law: certain forms of algorithmic bias will constitute direct discrimination [1]. In this paper, we explore the ramifications of this argument for existing taxonomies of machine bias and algorithmic fairness, how existing fairness metrics might need to be adapted, and potentially new measures may need to be introduced. We outline how the mappings between fairness measures and discrimination definitions implied hitherto may need to be revised and revisited.","['Direct Discrimination', 'Disparate Treatment', 'Bias', 'Fairness', 'Machine Learning']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Law']","['Reuben Binns', 'Jeremias Adams-Prassl', 'Aislinn Kelly-Lyth']","['Computer Science, University of Oxford', 'Law, University of Oxford', 'Law, University of Oxford']","['United Kingdom', 'United Kingdom', 'United Kingdom']"
https://doi.org/10.1145/3593013.3594093,Co-Designing for Transparency: Lessons from Building a Document Organization Tool in the Criminal Justice Domain,"Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice.","['Co-Design', 'Document Organization', 'User-Centered Design', 'Collaborative Design']","['Human-centered computing _ Interaction paradigms', 'Human-centered computing _ User studies', 'Computing methodologies _ Artificial intelligence', 'Computing methodologies _ Machine learning']","['Hellina Hailu Nigatu', 'Lisa Pickoff-White', 'John Canny', 'Sarah Chasins']","['EECS, UC Berkeley', 'KQED', 'EECS, UC Berkeley', 'EECS, UC Berkeley']","['USA', 'USA', 'USA', 'USA']"
https://doi.org/10.1145/3593013.3594058,Bias on Demand: A Modelling Framework That Generates Synthetic Data With Bias,"Nowadays, Machine Learning (ML) systems are widely used in various businesses and are increasingly being adopted to make decisions that can significantly impact people’s lives. However, these decision-making systems rely on data-driven learning, which poses a risk of propagating the bias embedded in the data. Despite various attempts by the algorithmic fairness community to outline different types of bias in data and algorithms, there is still a limited understanding of how these biases relate to the fairness of ML-based decision-making systems. In addition, efforts to mitigate bias and unfairness are often agnostic to the specific type(s) of bias present in the data. This paper explores the nature of fundamental types of bias, discussing their relationship to moral and technical frameworks. To prevent harmful consequences, it is essential to comprehend how and where bias is introduced throughout the entire modelling pipeline and possibly how to mitigate it. Our primary contribution is a framework for generating synthetic datasets with different forms of biases. We use our proposed synthetic data generator to perform experiments on different scenarios to showcase the interconnection between biases and their effect on performance and fairness evaluations. Furthermore, we provide initial insights into mitigating specific types of bias through post-processing techniques. The implementation of the synthetic data generator and experiments can be found at https://github.com/rcrupiISP/BiasOnDemand.","['bias', 'fairness', 'synthetic data', 'moral worldviews']","['Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence', 'Computing methodologies _ Machine learning', 'General and reference _ Metrics']","['Joachim Baumann', 'Alessandro Castelnovo', 'Riccardo Crupi', 'Nicole Inverardi', 'Daniele Regoli']","['Department of Informatics, University of Zurich, Switzerland and Zurich University of Applied Sciences', 'Data Science & Artificial Intelligence, Intesa Sanpaolo, Italy and Dept. of Informatics, Systems and Communication, University Milano Bicocca', 'Data Science & Artificial Intelligence, Intesa Sanpaolo', 'Data Science & Artificial Intelligence, Intesa Sanpaolo', 'Data Science & Artificial Intelligence, Intesa Sanpaolo']","['Switzerland', 'Italy', 'Italy', 'Italy', 'Italy']"
https://doi.org/10.1145/3593013.3594027,Power and Resistance in the Twitter Bias Discourse,"In 2020, the saliency-based image cropping tool deployed by Twitter to generate image previews was suspected of carrying a racial bias: Twitter users complained that Black people were systematically cropped out and, thus, made invisible by the cropping tool. As a response, Twitter conducted bias analyses, concluded that the cropping tool was indeed biased, and subsequently removed it. Soon after, Twitter hosted the first ""algorithmic bias bounty challenge"", inviting the general public to detect algorithmic harm in the cropping tool.  Twitter’s image cropping algorithm is a fascinating case study for exploring the push-and-pull dynamics of power relations between, firstly, algorithmic knowledge production inherent in machine learning systems, secondly, the bias discourse as resistance, and, thirdly, ensuing corporate responses as stabilization measures towards said resistance. In order to account for this three-part narrative of the case study, this paper is structured along the examination of the following three questions: (1) How is algorithmic, and especially, data-based knowledge production entrenched in power relations? (2) In what way does the discourse around bias serve as a vehicle for resistance against said power? Why and in what way is it effective? (3) How did Twitter as a company stabilize its position within and in relation to the bias discourse?  This paper explores these questions along the following steps: Section 2 lays out the interdisciplinary theoretical perspective of the analysis, combining, firstly, a mathematical-epistemic perspective that examines the mathematics underlying both machine learning systems and bias analyses with, secondly, Foucauldian concepts that make it possible to view mathematical tools as articulations of power relations. The subsequent three sections engage with the three questions posed above: Section 3, Power, is concerned with the first question, and it focuses on the algorithmic knowledge production in relation to Twitter’s cropping tool and its mathematical-epistemic foundations. Section 4, Resistance, addresses the second question, and it examines three bias analyses of the cropping tool, as well as their epistemic limitations, and it continues by conceptualizing the bias discourse in academic scholarship and activism as resistance to power. Section 5, Stabilization, engages with the third question, discussing Twitter’s response to the bias accusations and the way in which the company was able to effectively stabilize its position – rendering the bias discourse a vehicle for counter-resistance, too. This paper will be published in the open access volume Algorithmic Regimes: Methods, Interactions, and Politics (Amsterdam University Press, forthcoming), as well as on SSRN as a preprint.","['bias', 'power', 'discourse', 'resistance', 'Twitter', 'image cropping', 'Foucault']","['Computing methodologies _ Machine learning', 'Applied computing _ Law, social and behavioral sciences', 'Computing methodologies _ Philosophical/theoretical foundations of artificial intelligence']",['Paola Lopez'],"['University of Vienna, Austria and Weizenbaum Institute']",['Germany']
https://doi.org/10.1145/3593013.3594097,Personalized Pricing with Group Fairness Constraint,"In the big data era, personalized pricing has become a popular strategy that sets different prices for the same product according to individual customers’ features. Despite its popularity among companies, this practice is controversial due to the concerns over fairness that can be potentially caused by price discrimination. In this paper, we consider the problem of single-product personalized pricing for different groups under fairness constraints. Specifically, we define group fairness constraints under different distance metrics in the personalized pricing context. We then establish a stochastic formulation that maximizes the revenue. Under the discrete price setting, we reformulate this problem as a linear program and obtain the optimal pricing policy efficiently. To bridge the gap between the discrete and continuous price setting, theoretically, we prove a general gap between the optimal revenue with continuous and discrete price set of size l. Under some mild conditions, we improve this bound to . Empirically, we demonstrate the benefits of our approach over several baseline approaches on both synthetic data and real-world data. Our results also provide managerial insights into setting a proper fairness degree as well as an appropriate size of discrete price set.","['personalized pricing', 'group fairness', 'statistical parity', 'social welfare']","['Social and professional topics _ Computing / technology policy', 'Applied computing _ Electronic commerce', 'Applied computing _ Operations research']","['Xin Chen', 'Zexing Xu', 'Zishuo Zhao', 'Yuan Zhou']","['Georgia Institute of Technology', 'University of Illinois Urbana-Champaign', 'University of Illinois Urbana-Champaign', 'Tsinghua University']","['USA', 'USA', 'USA', 'China']"
https://doi.org/10.1145/3593013.3593987,In Her Shoes: Gendered Labelling in Crowdsourced Safety Perceptions Data from India,"In recent years, a proliferation of women’s safety mobile applications have emerged in India that crowdsource street safety perceptions to generate ‘safety maps’ used by policy makers for urban design and academics for studying mobility patterns. Men and women’s differential access to information and communication technologies (ICTs), however, and the distinctions between their social and cultural subjective experiences may mitigate the value of crowdsourced safety perceptions data and the predictive ability of machine learning (ML) models utilizing such data. We explore this by collecting and analyzing primary data on safety perceptions from New Delhi, India. Our curated dataset consists of streetviews covering a wide range of neighborhoods for which we obtain subjective safety ratings from both female and male respondents. Simulation experiments where varying the proportion of ratings from each gender are assumed missing demonstrate that the predictive ability of standard ML techniques relies crucially on the distribution of data producers. We find that obtaining large amounts of crowdsourced safety labels from male respondents for predicting female safety perceptions is inefficient in a number of scenarios and even undesirable in others. Detailed comparisons between female and male respondents’ data demonstrate significant gender differences in safety perceptions and associated vocabularies. Our results have important implications for the design of platforms relying on crowdsourced data and the insights generated from them.","['crowdsourced ratings', 'safety', 'gender', 'algorithmic bias', 'India']","['Human-centered computing _ HCI design and evaluation methods', 'Human-centered computing _ Empirical studies in collaborative and social computing']","['Nandana Sengupta', 'Ashwini Vaidya', 'James Evans']","['Indian Institute of Technology Delhi', 'Indian Institute of Technology Delhi', 'University of Chicago']","['India', 'India', 'USA']"
https://doi.org/10.1145/3593013.3594014,Algorithms as Social-Ecological-Technological Systems: An Environmental Justice Lens on Algorithmic Audits,"This paper reframes algorithmic systems as intimately connected to and part of social and ecological systems, and proposes a first-of-its-kind methodology for environmental justice-oriented algorithmic audits. How do we consider environmental and climate justice dimensions of the way algorithmic systems are designed, developed, and deployed? These impacts are inherently emergent and can only be understood and addressed at the level of relations between an algorithmic system and the social (including institutional) and ecological components of the broader ecosystem it operates in. As a result, we claim that in absence of an integral ontology for algorithmic systems, we cannot do justice to the emergent nature of broader environmental impacts of algorithmic systems and their underlying computational infrastructure. Furthermore, an integral lens provides many lessons from the history of environmental justice that are of relevance in current day struggles for algorithmic justice. We propose to define algorithmic systems as ontologically indistinct from Social-Ecological-Technological Systems (SETS), framing emergent implications as couplings between social, ecological, and technical components of the broader fabric in which algorithms are integrated and operate. We draw upon prior work on SETS analysis as well as emerging themes in the literature and practices of Environmental Justice (EJ) to conceptualize and assess algorithmic impact. We then offer three policy recommendations to help establish a SETS-based EJ approach to algorithmic audits: (1) broaden the inputs and open-up the outputs of an audit, (2) enable meaningful access to redress, and (3) guarantee a place-based and relational approach to the process of evaluating impact. We operationalize these as a qualitative framework of questions for a spectrum of stakeholders. Doing so, this article aims to inspire stronger and more frequent interactions across policymakers, researchers, practitioners, civil society, and grassroots communities. https://arxiv.org/abs/2305.05733.",[],"['Social and professional topics _ Computing / technology policy', 'Social and professional topics _ Technology audits', 'Software and its engineering _ Software development process management']","['Bogdana Rakova', 'Roel Dobbe']","['Mozilla Foundation', 'Delft University of Technology']","['USA', 'Netherlands']"
https://doi.org/10.1145/3593013.3594066,Emotions and Dynamic Assemblages: A Study of Automated Social Security Using Qualitative Longitudinal Research,"In this paper we argue that qualitative longitudinal research (QLLR) is a crucial research method for studying automated decision-making (ADM) systems as complex, dynamic digital assemblages. QLLR provides invaluable insight into the lived experiences of users as data subjects of ADMs as well as into the broader digital assemblage in which these systems operate. To demonstrate the utility of this method, we draw on an ongoing, empirical study examining Universal Credit (UC), an automated social security payment used in the United Kingdom. UC is digital-by-default and uses a dynamic, means-testing payment system to determine the monthly amount of claim people are entitled to.  We first provide a brief overview of the key epistemological challenges of studying ADMs before situating our study in relation to existing qualitative analyses of ADMs and their users, as well as qualitative longitudinal research. We highlight that, thus far, QLLR has been severely under-utilized in studying ADM systems. After a brief description of our study, aims and methodology, we present our findings illustrated through empirical cases that demonstrate the potential of QLLR in this area.  Overall, we argue that QLLR provides a unique opportunity to gather information on ADMs, both over time and in real time. Capturing information real-time allows for more granular accounts and provides an opportunity for gathering in situ data on emotions and attitudes of users and data subjects. The ability to record qualitative data over time has the potential to capture dynamic trajectories, including the fluctuations and uncertainties comprising users’ lived experiences. Through the personal accounts of data subjects, QLLR also gives researchers insight into how the emotional dimensions of users’ interactions with ADMs shapes their actions responding to these systems.","['Automated Social Security', 'Digital Social Security', 'Qualitative Research', 'Longitudinal Research', 'Interviews']","['Social and professional topics', 'Social and professional topics _ Computing / technology policy']","['Morgan Currie', 'Lena Podoletz']","['University of Edinburgh', 'University of Edinburgh']","['United Kingdom', 'United Kingdom']"